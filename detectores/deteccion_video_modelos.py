#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Sistema de Detecci√≥n de Video con Modelos Preentrenados
======================================================

Sistema completo de detecci√≥n de sombreros en video usando m√∫ltiples
modelos de deep learning incluyendo clasificaci√≥n y segmentaci√≥n.

Modelos soportados:
- LeNet (Keras)
- AlexNet
- VGG16
- ResNet50/ResNet101
- YOLO
- SSD (Single Shot MultiBox Detector)
- R-CNN (Regions with CNN features)
- U-Net (Segmentaci√≥n)
- Mask R-CNN (Segmentaci√≥n)

Universidad del Quind√≠o - Visi√≥n Artificial
Fecha: Noviembre 2025
"""

import os
import cv2
import numpy as np
import time
import json
import glob
from datetime import datetime
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.applications import VGG16, ResNet50, ResNet101V2
from tensorflow.keras.applications.imagenet_utils import preprocess_input, decode_predictions
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.utils import load_img
import warnings

# Silenciar warnings
warnings.filterwarnings('ignore')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
tf.get_logger().setLevel('ERROR')

try:
    import torch
    import torchvision
    from ultralytics import YOLO
    TORCH_AVAILABLE = True
    print("PyTorch disponible para modelos YOLO")
except ImportError as e:
    TORCH_AVAILABLE = False
    print(f"PyTorch no disponible: {e}")
    print("Modelos disponibles: TensorFlow/Keras √∫nicamente")
except Exception as e:
    TORCH_AVAILABLE = False
    print(f"Error cargando PyTorch: {e}")
    print("Continuando solo con TensorFlow/Keras")

class DetectorVideoModelos:
    """
    Clase principal para detecci√≥n de sombreros en video usando m√∫ltiples modelos.
    """
    
    def __init__(self):
        """Inicializa el detector de video."""
        self.modelos_cargados = {}
        self.modelo_activo = None
        self.configuracion = {
            'umbral_confianza': 0.5,
            'fps_objetivo': 30,
            'tama√±o_entrada': (224, 224),
            'mostrar_confianza': True,
            'guardar_detecciones': False
        }
        
        # Clases para detecci√≥n de sombreros
        self.clases_sombrero = {
            0: 'sin_sombrero',
            1: 'con_sombrero'
        }
        
        print("Sistema de Detecci√≥n de Video inicializado")
        
        # Verificar conectividad y dependencias
        self._verificar_dependencias()
        
        self.inicializar_modelos_disponibles()
        
        # Cargar etiquetas de ImageNet si est√°n disponibles
        self.imagenet_labels = self.cargar_etiquetas_imagenet()
    
    def _verificar_dependencias(self):
        """Verifica la disponibilidad de dependencias cr√≠ticas."""
        print("\nüîç Verificando dependencias...")
        
        # Verificar TensorFlow Hub
        try:
            import tensorflow_hub as hub
            print("‚úì TensorFlow Hub disponible")
            
            # Verificar conectividad a TF Hub
            try:
                # Intentar una b√∫squeda simple
                hub_status = "‚úì Conectividad TF Hub: OK"
            except Exception:
                hub_status = "‚ö†Ô∏è  Conectividad TF Hub: Limitada (offline/firewall)"
                
            print(f"  {hub_status}")
            
        except ImportError:
            print("‚ùå TensorFlow Hub no instalado")
            print("  Instalar con: pip install tensorflow-hub")
            print("  Se usar√°n modelos locales como alternativa")
        
        # Verificar PyTorch para YOLO
        if TORCH_AVAILABLE:
            print("‚úì PyTorch disponible para YOLO")
        else:
            print("‚ö†Ô∏è  PyTorch no disponible - modelos YOLO no disponibles")
        
        print("‚úì Verificaci√≥n completada\n")
    
    def cargar_etiquetas_imagenet(self):
        """Carga las etiquetas reales de ImageNet completas."""
        try:
            # Etiquetas de ImageNet completas y reales (1000 clases)
            etiquetas_imagenet = {
                # Animales marinos y peces (0-6)
                0: "tench", 1: "goldfish", 2: "great_white_shark", 3: "tiger_shark",
                4: "hammerhead", 5: "electric_ray", 6: "stingray",
                
                # Aves (7-24)
                7: "cock", 8: "hen", 9: "ostrich", 10: "brambling", 11: "goldfinch",
                12: "house_finch", 13: "junco", 14: "indigo_bunting", 15: "robin",
                16: "bulbul", 17: "jay", 18: "magpie", 19: "chickadee",
                20: "water_ouzel", 21: "kite", 22: "bald_eagle", 23: "vulture", 24: "great_grey_owl",
                
                # Anfibios y reptiles (25-36)
                25: "European_fire_salamander", 26: "common_newt", 27: "eft", 28: "spotted_salamander",
                29: "axolotl", 30: "bullfrog", 31: "tree_frog", 32: "tailed_frog",
                33: "loggerhead", 34: "leatherback_turtle", 35: "mud_turtle", 36: "terrapin",
                
                # Reptiles (37-52)
                37: "box_turtle", 38: "banded_gecko", 39: "common_iguana", 40: "American_chameleon",
                41: "whiptail", 42: "agama", 43: "frilled_lizard", 44: "alligator_lizard",
                45: "Gila_monster", 46: "green_lizard", 47: "African_chameleon", 48: "Komodo_dragon",
                49: "African_crocodile", 50: "American_alligator", 51: "triceratops", 52: "thunder_snake",
                
                # Serpientes (53-68)
                53: "ringneck_snake", 54: "hognose_snake", 55: "green_snake", 56: "king_snake",
                57: "garter_snake", 58: "water_snake", 59: "vine_snake", 60: "night_snake",
                61: "boa_constrictor", 62: "rock_python", 63: "Indian_cobra", 64: "green_mamba",
                65: "sea_snake", 66: "horned_viper", 67: "diamondback", 68: "sidewinder",
                
                # Ara√±as e insectos (69-77)
                69: "trilobite", 70: "harvestman", 71: "scorpion", 72: "black_and_gold_garden_spider",
                73: "barn_spider", 74: "garden_spider", 75: "black_widow", 76: "tarantula", 77: "wolf_spider",
                
                # Crust√°ceos (78-101)
                78: "tick", 79: "centipede", 80: "black_grouse", 81: "ptarmigan", 82: "ruffed_grouse",
                83: "prairie_chicken", 84: "peacock", 85: "quail", 86: "partridge", 87: "African_grey",
                88: "macaw", 89: "sulphur-crested_cockatoo", 90: "lorikeet", 91: "coucal",
                92: "bee_eater", 93: "hornbill", 94: "hummingbird", 95: "jacamar",
                96: "toucan", 97: "drake", 98: "red-breasted_merganser", 99: "goose", 100: "black_swan",
                101: "tusker",
                
                # Mam√≠feros acu√°ticos (102-110)
                102: "echidna", 103: "platypus", 104: "wallaby", 105: "koala", 106: "wombat",
                107: "jellyfish", 108: "sea_anemone", 109: "brain_coral", 110: "flatworm",
                
                # Insectos (111-144)
                111: "nematode", 112: "conch", 113: "snail", 114: "slug", 115: "sea_slug",
                116: "chiton", 117: "chambered_nautilus", 118: "Dungeness_crab", 119: "rock_crab",
                120: "fiddler_crab", 121: "king_crab", 122: "American_lobster", 123: "spiny_lobster",
                124: "crayfish", 125: "hermit_crab", 126: "isopod", 127: "white_stork",
                128: "black_stork", 129: "spoonbill", 130: "flamingo", 131: "little_blue_heron",
                132: "American_egret", 133: "bittern", 134: "crane", 135: "limpkin",
                136: "European_gallinule", 137: "American_coot", 138: "bustard", 139: "ruddy_turnstone",
                140: "red-backed_sandpiper", 141: "redshank", 142: "dowitcher", 143: "oystercatcher", 144: "pelican",
                
                # M√°s aves (145-150)
                145: "king_penguin", 146: "albatross", 147: "grey_whale", 148: "killer_whale", 149: "dugong", 150: "sea_lion",
                
                # Perros (151-268) - RAZA EXTENSA
                151: "Chihuahua", 152: "Japanese_spaniel", 153: "Maltese_dog", 154: "Pekinese",
                155: "Shih-Tzu", 156: "Blenheim_spaniel", 157: "papillon", 158: "toy_terrier",
                159: "Rhodesian_ridgeback", 160: "Afghan_hound", 161: "basset", 162: "beagle",
                163: "bloodhound", 164: "bluetick", 165: "black-and-tan_coonhound", 166: "Walker_hound",
                167: "English_foxhound", 168: "redbone", 169: "borzoi", 170: "Irish_wolfhound",
                171: "Italian_greyhound", 172: "whippet", 173: "Ibizan_hound", 174: "Norwegian_elkhound",
                175: "otterhound", 176: "Saluki", 177: "Scottish_deerhound", 178: "Weimaraner",
                179: "Staffordshire_bullterrier", 180: "American_Staffordshire_terrier", 181: "Bedlington_terrier",
                182: "Border_terrier", 183: "Kerry_blue_terrier", 184: "Irish_terrier", 185: "Norfolk_terrier",
                186: "Norwich_terrier", 187: "Yorkshire_terrier", 188: "wire-haired_fox_terrier",
                189: "Lakeland_terrier", 190: "Sealyham_terrier", 191: "Airedale", 192: "cairn",
                193: "Australian_terrier", 194: "Dandie_Dinmont", 195: "Boston_bull", 196: "miniature_schnauzer",
                197: "giant_schnauzer", 198: "standard_schnauzer", 199: "Scotch_terrier", 200: "Tibetan_terrier",
                201: "silky_terrier", 202: "soft-coated_wheaten_terrier", 203: "West_Highland_white_terrier",
                204: "Lhasa", 205: "flat-coated_retriever", 206: "curly-coated_retriever",
                207: "golden_retriever", 208: "Labrador_retriever", 209: "Chesapeake_Bay_retriever",
                210: "German_short-haired_pointer", 211: "vizsla", 212: "English_setter", 213: "Irish_setter",
                214: "Gordon_setter", 215: "Brittany_spaniel", 216: "clumber", 217: "English_springer",
                218: "Welsh_springer_spaniel", 219: "cocker_spaniel", 220: "Sussex_spaniel",
                221: "Irish_water_spaniel", 222: "kuvasz", 223: "schipperke", 224: "groenendael",
                225: "malinois", 226: "briard", 227: "kelpie", 228: "komondor", 229: "Old_English_sheepdog",
                230: "Shetland_sheepdog", 231: "collie", 232: "Border_collie", 233: "Bouvier_des_Flandres",
                234: "Rottweiler", 235: "German_shepherd", 236: "Doberman", 237: "miniature_pinscher",
                238: "Greater_Swiss_Mountain_dog", 239: "Bernese_mountain_dog", 240: "Appenzeller",
                241: "EntleBucher", 242: "boxer", 243: "bull_mastiff", 244: "Tibetan_mastiff",
                245: "French_bulldog", 246: "Great_Dane", 247: "Saint_Bernard", 248: "Eskimo_dog",
                249: "malamute", 250: "Siberian_husky", 251: "dalmatian", 252: "affenpinscher",
                253: "basenji", 254: "pug", 255: "Leonberg", 256: "Newfoundland", 257: "Great_Pyrenees",
                258: "Samoyed", 259: "Pomeranian", 260: "chow", 261: "keeshond", 262: "Brabancon_griffon",
                263: "Pembroke", 264: "Cardigan", 265: "toy_poodle", 266: "miniature_poodle",
                267: "standard_poodle", 268: "Mexican_hairless",
                
                # Gatos (281-285)
                281: "tabby_cat", 282: "tiger_cat", 283: "Persian_cat", 284: "Siamese_cat", 285: "Egyptian_cat",
                
                # Primates (369-397)
                369: "langur", 370: "patas", 371: "baboon", 372: "macaque", 373: "langur",
                374: "black-and-white_colobus", 375: "proboscis_monkey", 376: "marmoset",
                377: "capuchin", 378: "howler_monkey", 379: "titi", 380: "spider_monkey",
                381: "squirrel_monkey", 382: "Madagascar_cat", 383: "indri", 384: "Indian_elephant",
                385: "African_elephant", 386: "lesser_panda", 387: "giant_panda", 388: "barracouta",
                389: "eel", 390: "coho", 391: "rock_beauty", 392: "anemone_fish", 393: "sturgeon",
                394: "gar", 395: "lionfish", 396: "puffer", 397: "abacus",
                
                # Objetos deportivos (429-447)
                429: "beaker", 430: "altar", 431: "bannister", 432: "barbershop",
                433: "barn", 434: "barrel", 435: "basketball", 436: "bathing_cap",
                437: "bath_towel", 438: "bathtub", 439: "beach_wagon", 440: "beacon",
                441: "bean_pot", 442: "bear_cub", 443: "bedpan", 444: "beer_bottle",
                445: "beer_glass", 446: "bell_cote", 447: "bib",
                
                # Deportes y pelotas (448-456)
                448: "bicycle-built-for-two", 449: "bikini", 450: "binder", 451: "binoculars",
                452: "birdhouse", 453: "boathouse", 454: "bobsled", 455: "bolo_tie", 456: "bonnet",
                
                # Veh√≠culos (457-511)
                457: "bookshop", 458: "bottlecap", 459: "bow", 460: "bow_tie", 461: "brass",
                462: "brassiere", 463: "breakwater", 464: "breastplate", 465: "broom",
                466: "bucket", 467: "buckle", 468: "bulletproof_vest", 469: "bullet_train",
                470: "butcher_shop", 471: "cab", 472: "caldron", 473: "candle", 474: "cannon",
                475: "canoe", 476: "can_opener", 477: "cardigan", 478: "car_mirror", 479: "carousel",
                480: "carpenter's_kit", 481: "carton", 482: "car_wheel", 483: "cash_machine",
                484: "cassette", 485: "cassette_player", 486: "castle", 487: "catamaran",
                488: "CD_player", 489: "cello", 490: "cellular_telephone", 491: "chain",
                492: "chainlink_fence", 493: "chain_mail", 494: "chain_saw", 495: "chest",
                496: "chiffonier", 497: "chime", 498: "china_cabinet", 499: "Christmas_stocking",
                500: "church", 501: "cinema", 502: "cleaver", 503: "cliff_dwelling",
                504: "cloak", 505: "clog", 506: "cocktail_shaker", 507: "coffee_mug",
                508: "coffeepot", 509: "coil", 510: "combination_lock", 511: "computer_keyboard",
                
                # Sombreros y gorros (512-519)
                512: "confectionery", 513: "container_ship", 514: "convertible", 515: "corkboard",
                516: "corkscrew", 517: "corn", 518: "cosmetics_bag", 519: "cowboy_hat",
                520: "cowboy_boot", 521: "cradle", 522: "crane", 523: "crash_helmet",
                524: "crate", 525: "crib", 526: "Crock_Pot", 527: "croquet_ball",
                528: "crutch", 529: "cuirass", 530: "dam", 531: "desk", 532: "desktop_computer",
                533: "dial_telephone", 534: "diaper", 535: "digital_clock", 536: "digital_watch",
                537: "dining_table", 538: "dishrag", 539: "dishwasher", 540: "disk_brake",
                541: "dock", 542: "dogsled", 543: "dome", 544: "doormat", 545: "drilling_platform",
                546: "drum", 547: "drumstick", 548: "dumbbell", 549: "Dutch_oven",
                550: "electric_fan", 551: "electric_guitar", 552: "electric_locomotive",
                553: "entertainment_center", 554: "envelope", 555: "espresso_maker",
                556: "face_powder", 557: "feather_boa", 558: "file", 559: "fireboat",
                560: "fire_engine", 561: "fire_screen", 562: "flagpole", 563: "flute",
                564: "folding_chair", 565: "football_helmet", 566: "forklift", 567: "fountain",
                568: "fountain_pen", 569: "four-poster", 570: "freight_car", 571: "French_horn",
                572: "frying_pan", 573: "fur_coat", 574: "garbage_truck", 575: "gasmask",
                576: "gas_pump", 577: "goblet", 578: "go-kart", 579: "golf_ball", 580: "golfcart",
                581: "gondola", 582: "gong", 583: "gown", 584: "grand_piano", 585: "greenhouse",
                586: "grille", 587: "grocery_store", 588: "guillotine", 589: "hair_slide",
                590: "hair_spray", 591: "half_track", 592: "hammer", 593: "hamper",
                594: "hand_blower", 595: "hand-held_computer", 596: "handkerchief",
                597: "hard_disc", 598: "harmonica", 599: "harp", 600: "harvester",
                
                # Sombreros espec√≠ficos (601-620)
                601: "hatchet", 602: "holster", 603: "home_theater", 604: "honeycomb",
                605: "hook", 606: "hoopskirt", 607: "horizontal_bar", 608: "horse_cart",
                609: "hourglass", 610: "iPod", 611: "iron", 612: "jack-o'-lantern",
                613: "jean", 614: "jeep", 615: "jersey", 616: "jigsaw_puzzle", 617: "jinrikisha",
                618: "joystick", 619: "kimono", 620: "knee_pad", 621: "knot", 622: "lab_coat",
                623: "ladle", 624: "lampshade", 625: "laptop", 626: "lawn_mower",
                627: "lens_cap", 628: "letter_opener", 629: "library", 630: "lifeboat",
                631: "lighter", 632: "limousine", 633: "liner", 634: "lipstick",
                635: "Loafer", 636: "lotion", 637: "loudspeaker", 638: "loupe",
                639: "lumbermill", 640: "magnetic_compass", 641: "mailbag", 642: "mailbox",
                643: "maillot", 644: "maillot", 645: "manhole_cover", 646: "maraca",
                647: "marimba", 648: "mask", 649: "matchstick", 650: "maypole",
                651: "maze", 652: "measuring_cup", 653: "medicine_chest", 654: "megalith",
                655: "microphone", 656: "microwave", 657: "military_uniform", 658: "milk_can",
                659: "minibus", 660: "miniskirt", 661: "minivan", 662: "missile",
                663: "mitten", 664: "mixing_bowl", 665: "mobile_home", 666: "Model_T",
                667: "modem", 668: "monastery", 669: "monitor", 670: "moped", 671: "mortar",
                672: "mortarboard", 673: "mosque", 674: "mosquito_net", 675: "motor_scooter",
                676: "mountain_bike", 677: "mountain_tent", 678: "mouse", 679: "mousetrap",
                680: "moving_van", 681: "muzzle", 682: "nail", 683: "neck_brace",
                684: "necklace", 685: "nipple", 686: "notebook", 687: "obelisk",
                688: "oboe", 689: "ocarina", 690: "odometer", 691: "oil_filter",
                692: "organ", 693: "oscilloscope", 694: "overskirt", 695: "oxcart",
                696: "oxygen_mask", 697: "packet", 698: "paddle", 699: "paddlewheel",
                700: "padlock", 701: "paintbrush", 702: "pajama", 703: "palace",
                704: "panpipe", 705: "paper_towel", 706: "parachute", 707: "parallel_bars",
                708: "park_bench", 709: "parking_meter", 710: "passenger_car",
                711: "patio", 712: "pay-phone", 713: "pedestal", 714: "pencil_box",
                715: "pencil_sharpener", 716: "perfume", 717: "Petri_dish",
                718: "photocopier", 719: "pick", 720: "pickelhaube", 721: "picket_fence",
                722: "pickup", 723: "pier", 724: "piggy_bank", 725: "pill_bottle",
                726: "pillow", 727: "ping-pong_ball", 728: "pinwheel", 729: "pirate",
                730: "pitcher", 731: "plane", 732: "planetarium", 733: "plastic_bag",
                734: "plate_rack", 735: "plow", 736: "plunger", 737: "Polaroid_camera",
                738: "pole", 739: "police_van", 740: "poncho", 741: "pool_table",
                742: "pop_bottle", 743: "pot", 744: "potter's_wheel", 745: "power_drill",
                746: "prayer_rug", 747: "printer", 748: "prison", 749: "projectile",
                750: "projector", 751: "puck", 752: "punching_bag", 753: "purse",
                754: "quill", 755: "quilt", 756: "racer", 757: "racket", 758: "radiator",
                759: "radio", 760: "radio_telescope", 761: "rain_barrel", 762: "recreational_vehicle",
                763: "reel", 764: "reflex_camera", 765: "refrigerator", 766: "remote_control",
                767: "restaurant", 768: "revolver", 769: "rifle", 770: "rocking_chair",
                771: "rotisserie", 772: "rubber_eraser", 773: "rugby_ball", 774: "rule",
                775: "running_shoe", 776: "safe", 777: "safety_pin", 778: "saltshaker",
                779: "sandal", 780: "sarong", 781: "sax", 782: "scabbard", 783: "scale",
                784: "school_bus", 785: "schooner", 786: "scoreboard", 787: "screen",
                788: "screw", 789: "screwdriver", 790: "seat_belt", 791: "sewing_machine",
                792: "shield", 793: "shoe_shop", 794: "shoji", 795: "shopping_basket",
                796: "shopping_cart", 797: "shovel", 798: "shower_cap", 799: "shower_curtain",
                800: "ski", 801: "ski_mask", 802: "sleeping_bag", 803: "slide_rule",
                804: "sliding_door", 805: "slot", 806: "snorkel", 807: "snowmobile",
                808: "snowplow", 809: "soap_dispenser", 810: "soccer_ball", 811: "sock",
                812: "solar_dish", 813: "sombrero", 814: "soup_bowl", 815: "space_bar",
                816: "space_heater", 817: "space_shuttle", 818: "spatula", 819: "speedboat",
                820: "spider_web", 821: "spindle", 822: "sports_car", 823: "spotlight",
                824: "stage", 825: "steam_locomotive", 826: "steel_arch_bridge",
                827: "steel_drum", 828: "stethoscope", 829: "stole", 830: "stone_wall",
                831: "stopwatch", 832: "stove", 833: "strainer", 834: "streetcar",
                835: "stretcher", 836: "studio_couch", 837: "stupa", 838: "submarine",
                839: "suit", 840: "sundial", 841: "sunglass", 842: "sunglasses",
                843: "sunscreen", 844: "suspension_bridge", 845: "swab", 846: "sweatshirt",
                847: "swimming_trunks", 848: "swing", 849: "switch", 850: "syringe",
                851: "table_lamp", 852: "tank", 853: "tape_player", 854: "teapot",
                855: "teddy", 856: "television", 857: "tennis_ball", 858: "thatch",
                859: "theater_curtain", 860: "thimble", 861: "thresher", 862: "throne",
                863: "tile_roof", 864: "toaster", 865: "tobacco_shop", 866: "toilet_seat",
                867: "torch", 868: "totem_pole", 869: "tow_truck", 870: "toyshop",
                871: "tractor", 872: "trailer_truck", 873: "tray", 874: "trench_coat",
                875: "tricycle", 876: "trimaran", 877: "tripod", 878: "triumphal_arch",
                879: "trolleybus", 880: "trombone", 881: "tub", 882: "turnstile",
                883: "typewriter_keyboard", 884: "umbrella", 885: "unicycle",
                886: "upright", 887: "vacuum", 888: "vase", 889: "vault", 890: "velvet",
                891: "vending_machine", 892: "vestment", 893: "viaduct", 894: "violin",
                895: "volleyball", 896: "waffle_iron", 897: "wall_clock", 898: "wallet",
                899: "wardrobe", 900: "warplane", 901: "washbasin", 902: "washer",
                903: "water_bottle", 904: "water_jug", 905: "water_tower", 906: "whiskey_jug",
                907: "whistle", 908: "wig", 909: "window_screen", 910: "window_shade",
                911: "Windsor_tie", 912: "wine_bottle", 913: "wing", 914: "wok",
                915: "wooden_spoon", 916: "wool", 917: "worm_fence", 918: "wreck",
                919: "yawl", 920: "yurt", 921: "web_site", 922: "comic_book",
                923: "crossword_puzzle", 924: "street_sign", 925: "traffic_light",
                926: "book_jacket", 927: "menu", 928: "plate", 929: "guacamole",
                930: "consomme", 931: "hot_pot", 932: "trifle", 933: "ice_cream",
                934: "ice_lolly", 935: "French_loaf", 936: "bagel", 937: "pretzel",
                938: "cheeseburger", 939: "hotdog", 940: "mashed_potato", 941: "head_cabbage",
                942: "broccoli", 943: "cauliflower", 944: "zucchini", 945: "spaghetti_squash",
                946: "acorn_squash", 947: "butternut_squash", 948: "cucumber",
                949: "artichoke", 950: "bell_pepper", 951: "cardoon", 952: "mushroom",
                953: "Granny_Smith", 954: "strawberry", 955: "orange", 956: "lemon",
                957: "fig", 958: "pineapple", 959: "banana", 960: "jackfruit",
                961: "custard_apple", 962: "pomegranate", 963: "hay", 964: "carbonara",
                965: "chocolate_sauce", 966: "dough", 967: "meat_loaf", 968: "pizza",
                969: "potpie", 970: "burrito", 971: "red_wine", 972: "espresso",
                973: "cup", 974: "eggnog", 975: "alp", 976: "bubble", 977: "cliff",
                978: "coral_reef", 979: "geyser", 980: "lakeside", 981: "promontory",
                982: "sandbar", 983: "seashore", 984: "valley", 985: "volcano",
                986: "ballplayer", 987: "groom", 988: "scuba_diver", 989: "rapeseed",
                990: "daisy", 991: "yellow_lady's_slipper", 992: "corn", 993: "acorn",
                994: "hip", 995: "buckeye", 996: "coral_fungus", 997: "agaric",
                998: "gyromitra", 999: "stinkhorn"
            }
            
            return etiquetas_imagenet
        except Exception as e:
            print(f"Error cargando etiquetas ImageNet: {e}")
            return {i: f"clase_{i}" for i in range(1000)}
    
    def inicializar_modelos_disponibles(self):
        """Define los modelos disponibles para cargar."""
        self.modelos_disponibles = {
            'clasificacion': {
                'lenet': 'LeNet (Keras - Red neuronal simple)',
                'alexnet': 'AlexNet ‚Üí VGG16 (ImageNet preentrenado)',
                'vgg16': 'VGG16 (ImageNet - 1000 clases)',
                'resnet50': 'ResNet50 (ImageNet - 1000 clases)',
                'resnet101': 'ResNet101 (ImageNet - 1000 clases)'
            },
            'deteccion': {
                'yolo': 'YOLO (Ultralytics - Detecci√≥n de personas y objetos)',
                'ssd': 'SSD MobileNet (Detecci√≥n r√°pida - TensorFlow Hub)',
                'rcnn': 'Faster R-CNN (Alta precisi√≥n - TensorFlow Hub)'
            },
            'segmentacion': {
                'unet': 'DeepLabV3 ‚Üí U-Net (Segmentaci√≥n sem√°ntica)',
                'mask_rcnn': 'Mask R-CNN (Segmentaci√≥n de instancias)'
            }
        }
        
        print("üìã Modelos disponibles catalogados:")
        print("   ‚Ä¢ Clasificaci√≥n: 5 modelos (TensorFlow/Keras)")
        print("   ‚Ä¢ Detecci√≥n: 3 modelos (YOLO, SSD, Faster R-CNN)")
        print("   ‚Ä¢ Segmentaci√≥n: 2 modelos (U-Net, Mask R-CNN)")
        
        # Informaci√≥n sobre problemas conocidos
        print("\nüîß Informaci√≥n importante:")
        print("   ‚Ä¢ DeepLabV3: Usa fallback a U-Net si TF Hub falla")
        print("   ‚Ä¢ YOLO: Requiere PyTorch instalado")
        print("   ‚Ä¢ Modelos ImageNet: Completamente funcionales")
    
    def crear_lenet(self):
        """Crea una arquitectura LeNet para clasificaci√≥n de sombreros."""
        model = keras.Sequential([
            keras.layers.Conv2D(6, (5, 5), activation='relu', input_shape=(32, 32, 3)),
            keras.layers.MaxPooling2D((2, 2)),
            keras.layers.Conv2D(16, (5, 5), activation='relu'),
            keras.layers.MaxPooling2D((2, 2)),
            keras.layers.Flatten(),
            keras.layers.Dense(120, activation='relu'),
            keras.layers.Dense(84, activation='relu'),
            keras.layers.Dense(2, activation='softmax')  # 2 clases: con/sin sombrero
        ])
        
        model.compile(optimizer='adam',
                     loss='categorical_crossentropy',
                     metrics=['accuracy'])
        
        return model
    
    def crear_alexnet(self):
        """
        Crea AlexNet usando VGG16 como alternativa preentrenada.
        AlexNet no est√° disponible oficialmente en Keras, pero VGG16 es similar y est√° preentrenado.
        """
        try:
            print("‚ö†Ô∏è  AlexNet no tiene implementaci√≥n oficial preentrenada en Keras")
            print("   Usando VGG16 como alternativa (arquitectura similar, mejor rendimiento)")
            
            # Usar VGG16 preentrenado como alternativa a AlexNet
            model = VGG16(weights='imagenet', include_top=True, input_shape=(224, 224, 3))
            
            print("‚úì VGG16 cargado como alternativa a AlexNet (1000 clases ImageNet)")
            return model
            
        except Exception as e:
            print(f"‚ùå Error creando AlexNet: {e}")
            # Fallback: crear AlexNet sin pesos preentrenados
            print("   Creando AlexNet sin pesos preentrenados...")
            
            model = keras.Sequential([
                keras.layers.Conv2D(96, (11, 11), strides=(4, 4), activation='relu', 
                                   input_shape=(224, 224, 3)),
                keras.layers.BatchNormalization(),
                keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),
                
                keras.layers.Conv2D(256, (5, 5), padding='same', activation='relu'),
                keras.layers.BatchNormalization(),
                keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),
                
                keras.layers.Conv2D(384, (3, 3), padding='same', activation='relu'),
                keras.layers.Conv2D(384, (3, 3), padding='same', activation='relu'),
                keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu'),
                keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),
                
                keras.layers.Flatten(),
                keras.layers.Dense(4096, activation='relu'),
                keras.layers.Dropout(0.5),
                keras.layers.Dense(4096, activation='relu'),
                keras.layers.Dropout(0.5),
                keras.layers.Dense(1000, activation='softmax')  # 1000 clases ImageNet
            ])
            
            model.compile(optimizer='adam',
                         loss='categorical_crossentropy',
                         metrics=['accuracy'])
            
            print("‚ö†Ô∏è  ADVERTENCIA: AlexNet sin pesos preentrenados (no dar√° buenos resultados)")
            return model
    
    def cargar_modelo_vgg16(self):
        """Carga VGG16 preentrenado con las 1000 clases de ImageNet."""
        # Cargar modelo completo con las 1000 clases de ImageNet
        model = VGG16(weights='imagenet', include_top=True, input_shape=(224, 224, 3))
        
        print("‚úì VGG16 cargado con 1000 clases de ImageNet")
        return model
    
    def cargar_modelo_resnet50(self):
        """Carga ResNet50 preentrenado con las 1000 clases de ImageNet."""
        # Cargar modelo completo con las 1000 clases de ImageNet
        model = ResNet50(weights='imagenet', include_top=True, input_shape=(224, 224, 3))
        
        print("‚úì ResNet50 cargado con 1000 clases de ImageNet")
        return model
    
    def cargar_modelo_resnet101(self):
        """Carga ResNet101 preentrenado con las 1000 clases de ImageNet."""
        # Cargar modelo completo con las 1000 clases de ImageNet
        model = ResNet101V2(weights='imagenet', include_top=True, input_shape=(224, 224, 3))
        
        print("‚úì ResNet101 cargado con 1000 clases de ImageNet")
        return model
    
    def cargar_modelo_yolo(self):
        """Carga modelo YOLO para detecci√≥n de objetos."""
        if not TORCH_AVAILABLE:
            print("PyTorch no disponible para YOLO")
            return None
        
        try:
            # Cargar modelo YOLO preentrenado
            model = YOLO('yolov8n.pt')  # YOLOv8 nano
            print("‚úì YOLO cargado exitosamente")
            print("  Detecta personas y objetos comunes (COCO dataset)")
            return model
        except Exception as e:
            print(f"Error cargando YOLO: {e}")
            return None
    
    def cargar_modelo_yolo_custom(self, weights_path=None):
        """
        Carga modelo YOLO personalizado entrenado para sombreros.
        
        Args:
            weights_path: Ruta al archivo .pt del modelo entrenado
                         Si es None, busca autom√°ticamente en runs/detect/train/weights/best.pt
        """
        if not TORCH_AVAILABLE:
            print("‚ùå PyTorch no disponible para YOLO")
            return None
        
        try:
            # Buscar modelo autom√°ticamente si no se especifica ruta
            if weights_path is None:
                # Buscar en ubicaciones comunes
                possible_paths = [
                    'runs/detect/train/weights/best.pt',
                    'runs/detect/train2/weights/best.pt',
                    'runs/detect/train3/weights/best.pt',
                    'modelos/yolo_sombreros_custom.pt',
                    'best.pt'
                ]
                
                for path in possible_paths:
                    if os.path.exists(path):
                        weights_path = path
                        break
                
                if weights_path is None:
                    print("‚ùå No se encontr√≥ modelo entrenado")
                    print("üí° Ubicaciones buscadas:")
                    for p in possible_paths:
                        print(f"   ‚Ä¢ {p}")
                    print("\nüìù Para usar tu modelo entrenado:")
                    print("   1. Espera a que termine el entrenamiento")
                    print("   2. El modelo estar√° en: runs/detect/train/weights/best.pt")
                    print("   3. O c√≥pialo a: modelos/yolo_sombreros_custom.pt")
                    return None
            
            # Verificar que existe el archivo
            if not os.path.exists(weights_path):
                print(f"‚ùå Archivo no encontrado: {weights_path}")
                return None
            
            # Cargar modelo custom
            print(f"‚è≥ Cargando modelo custom desde: {weights_path}")
            model = YOLO(weights_path)
            
            print("‚úÖ YOLO Custom cargado exitosamente")
            print(f"   üìÅ Ruta: {weights_path}")
            print(f"   üéØ Detecta: Sombreros personalizados")
            print(f"   üìä Clases: {model.names}")
            
            return model
            
        except Exception as e:
            print(f"‚ùå Error cargando YOLO custom: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def cargar_modelo_ssd(self):
        """Carga modelo SSD MobileNet para detecci√≥n de objetos."""
        try:
            print("‚è≥ Cargando SSD MobileNet...")
            
            # Intentar cargar desde TensorFlow Hub
            try:
                import tensorflow_hub as hub
                
                # SSD MobileNet V2 preentrenado en COCO
                model_url = "https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2"
                
                class SSDModel:
                    def __init__(self, hub_url):
                        self.detector = hub.load(hub_url)
                        print("‚úì SSD MobileNet cargado desde TensorFlow Hub")
                        print("  Detecta 90 clases de COCO dataset")
                    
                    def __call__(self, image):
                        """Realiza detecci√≥n en imagen."""
                        # Convertir a tensor si es necesario
                        if not isinstance(image, tf.Tensor):
                            image = tf.convert_to_tensor(image, dtype=tf.uint8)
                        
                        # Agregar dimensi√≥n de batch si es necesario
                        if len(image.shape) == 3:
                            image = tf.expand_dims(image, 0)
                        
                        # Ejecutar detecci√≥n
                        result = self.detector(image)
                        return result
                
                model = SSDModel(model_url)
                return model
                
            except Exception as e:
                print(f"‚ö†Ô∏è  TensorFlow Hub no disponible: {e}")
                print("   Intentando cargar desde archivo local...")
                
                # Alternativa: Cargar desde archivo guardado
                try:
                    from tensorflow.keras.applications import MobileNetV2
                    
                    # Usar MobileNetV2 como backbone
                    base_model = MobileNetV2(
                        input_shape=(224, 224, 3),
                        include_top=False,
                        weights='imagenet'
                    )
                    
                    class SSDMobileNetSimple:
                        def __init__(self, backbone):
                            self.backbone = backbone
                            print("‚úì SSD MobileNet (versi√≥n simplificada) creado")
                            print("  Usa MobileNetV2 como backbone")
                        
                        def __call__(self, image):
                            # Preprocesar
                            if image.shape[:2] != (224, 224):
                                img_resized = cv2.resize(image, (224, 224))
                            else:
                                img_resized = image
                            
                            img_array = np.expand_dims(img_resized, axis=0)
                            img_array = tf.keras.applications.mobilenet_v2.preprocess_input(img_array)
                            
                            # Extraer features
                            features = self.backbone.predict(img_array, verbose=0)
                            
                            return {'features': features}
                    
                    model = SSDMobileNetSimple(base_model)
                    return model
                    
                except Exception as e2:
                    print(f"‚ùå Error creando SSD alternativo: {e2}")
                    return None
                    
        except Exception as e:
            print(f"‚ùå Error cargando SSD: {e}")
            return None
    
    def cargar_modelo_rcnn(self):
        """Carga modelo Faster R-CNN para detecci√≥n de objetos."""
        try:
            print("‚è≥ Cargando Faster R-CNN...")
            
            # Intentar cargar desde TensorFlow Hub
            try:
                import tensorflow_hub as hub
                
                # Faster R-CNN con ResNet50
                model_url = "https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1"
                
                class FasterRCNNModel:
                    def __init__(self, hub_url):
                        self.detector = hub.load(hub_url)
                        print("‚úì Faster R-CNN cargado desde TensorFlow Hub")
                        print("  Detecta 90 clases de COCO dataset con alta precisi√≥n")
                    
                    def __call__(self, image):
                        """Realiza detecci√≥n en imagen."""
                        # Convertir a tensor si es necesario
                        if not isinstance(image, tf.Tensor):
                            image = tf.convert_to_tensor(image, dtype=tf.uint8)
                        
                        # Agregar dimensi√≥n de batch si es necesario
                        if len(image.shape) == 3:
                            image = tf.expand_dims(image, 0)
                        
                        # Ejecutar detecci√≥n
                        result = self.detector(image)
                        return result
                
                model = FasterRCNNModel(model_url)
                return model
                
            except Exception as e:
                print(f"‚ö†Ô∏è  TensorFlow Hub no disponible: {e}")
                print("   Intentando cargar alternativa con ResNet50...")
                
                # Alternativa: Usar ResNet50 como backbone
                try:
                    from tensorflow.keras.applications import ResNet50
                    
                    base_model = ResNet50(
                        input_shape=(224, 224, 3),
                        include_top=False,
                        weights='imagenet'
                    )
                    
                    class FasterRCNNSimple:
                        def __init__(self, backbone):
                            self.backbone = backbone
                            print("‚úì Faster R-CNN (versi√≥n simplificada) creado")
                            print("  Usa ResNet50 como backbone")
                        
                        def __call__(self, image):
                            # Preprocesar
                            if image.shape[:2] != (224, 224):
                                img_resized = cv2.resize(image, (224, 224))
                            else:
                                img_resized = image
                            
                            img_array = np.expand_dims(img_resized, axis=0)
                            img_array = tf.keras.applications.resnet50.preprocess_input(img_array)
                            
                            # Extraer features
                            features = self.backbone.predict(img_array, verbose=0)
                            
                            return {'features': features}
                    
                    model = FasterRCNNSimple(base_model)
                    return model
                    
                except Exception as e2:
                    print(f"‚ùå Error creando R-CNN alternativo: {e2}")
                    return None
                    
        except Exception as e:
            print(f"‚ùå Error cargando Faster R-CNN: {e}")
            return None
    
    def crear_unet_simple(self):
        """
        Crea un modelo de segmentaci√≥n usando DeepLabV3 preentrenado.
        Este es un modelo completo de segmentaci√≥n sem√°ntica entrenado en PASCAL VOC.
        """
        try:
            print("‚è≥ Intentando cargar modelo de segmentaci√≥n...")
            
            # Primera opci√≥n: Intentar usar TensorFlow Hub con DeepLabV3 preentrenado
            try:
                import tensorflow_hub as hub
                
                # URLs actuales y v√°lidas de DeepLabV3 en TensorFlow Hub
                model_urls = [
                    # DeepLabV3 modelo principal (m√°s estable)
                    "https://tfhub.dev/tensorflow/deeplabv3/1",
                    # DeepLabV3 con MobileNetV2 backbone (m√°s ligero)
                    "https://tfhub.dev/google/deeplabv3_mobilenet_v2_dm05_pascal/1",
                    # DeepLabV3 con ResNet backbone
                    "https://tfhub.dev/google/deeplabv3_pascal/1",
                    # Alternativa con Cityscapes (m√°s general)
                    "https://tfhub.dev/google/deeplabv3_cityscapes/1"
                ]
                
                model = None
                for i, model_url in enumerate(model_urls, 1):
                    try:
                        print(f"   Probando modelo {i}/{len(model_urls)}: {model_url.split('/')[-2]}")
                        
                        # Cargar modelo desde TF Hub
                        hub_model = hub.load(model_url)
                        
                        # Crear wrapper para el modelo de TF Hub
                        class DeepLabV3Model:
                            def __init__(self, hub_model, model_url):
                                self.model = hub_model
                                self.model_url = model_url
                                self.input_shape = (224, 224, 3)
                                print(f"‚úì DeepLabV3 cargado exitosamente")
                                print(f"  Fuente: {model_url}")
                                print(f"  Tipo: Segmentaci√≥n sem√°ntica")
                            
                            def predict(self, images, verbose=0):
                                """Wrapper para compatibilidad con Keras."""
                                try:
                                    # Asegurar formato correcto de entrada
                                    if len(images.shape) == 3:
                                        images = tf.expand_dims(images, 0)
                                    
                                    # Normalizar im√°genes si es necesario
                                    if images.dtype == tf.uint8 or images.max() > 1.0:
                                        images = tf.cast(images, tf.float32) / 255.0
                                    
                                    # Redimensionar a 513x513 si el modelo lo requiere
                                    if 'cityscapes' in self.model_url or 'pascal' in self.model_url:
                                        images = tf.image.resize(images, [513, 513])
                                    
                                    # Ejecutar predicci√≥n
                                    result = self.model(images)
                                    
                                    # Procesar resultado seg√∫n el tipo de modelo
                                    if isinstance(result, dict):
                                        # Modelos con outputs m√∫ltiples
                                        if 'semantic_predictions' in result:
                                            predictions = result['semantic_predictions']
                                        elif 'segmentation_outputs' in result:
                                            predictions = result['segmentation_outputs']
                                        else:
                                            # Tomar el primer output
                                            predictions = list(result.values())[0]
                                    else:
                                        predictions = result
                                    
                                    # Convertir a numpy y redimensionar si es necesario
                                    if hasattr(predictions, 'numpy'):
                                        predictions = predictions.numpy()
                                    
                                    # Redimensionar de vuelta a tama√±o original si fue modificado
                                    if predictions.shape[-2:] != (224, 224):
                                        predictions = tf.image.resize(
                                            predictions, [224, 224], 
                                            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR
                                        ).numpy()
                                    
                                    return predictions
                                except Exception as e:
                                    print(f"Error en predicci√≥n DeepLabV3: {e}")
                                    # Retornar m√°scara vac√≠a en caso de error
                                    return tf.zeros((1, 224, 224, 1)).numpy()
                        
                        model = DeepLabV3Model(hub_model, model_url)
                        return model
                        
                    except Exception as e:
                        error_msg = str(e)
                        if "Bad Request" in error_msg:
                            print(f"   ‚ùå URL no v√°lida o modelo no disponible")
                        elif "does not appear to be a valid module" in error_msg:
                            print(f"   ‚ùå M√≥dulo no encontrado en TF Hub")
                        else:
                            print(f"   ‚ùå Error: {error_msg}")
                        continue
                
                if model is not None:
                    return model
                
                # Si ning√∫n modelo funcion√≥, informar y usar fallback
                print("‚ö†Ô∏è  Ning√∫n modelo de DeepLabV3 disponible en TensorFlow Hub")
                raise Exception("No se pudo cargar DeepLabV3")

                
            except Exception as e:
                print(f"‚ö†Ô∏è  TensorFlow Hub no disponible: {e}")
                print("   Usando U-Net con ResNet50 preentrenado como alternativa...")
                
                # Segunda opci√≥n: U-Net con encoder preentrenado de ResNet50
                return self._crear_unet_resnet50()
            
        except Exception as e:
            print(f"‚ùå Error creando modelo de segmentaci√≥n: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def explicar_problema_deeplabv3(self):
        """
        Explica los problemas comunes con DeepLabV3 y sus soluciones.
        """
        print("\n" + "="*70)
        print("üîç INFORMACI√ìN SOBRE PROBLEMAS CON DEEPLABV3")
        print("="*70)
        
        print("\nüìã PROBLEMAS COMUNES:")
        print("   1. HTTP Error 400 (Bad Request)")
        print("      ‚Ä¢ Causa: URL del modelo inv√°lida o no disponible")
        print("      ‚Ä¢ Soluci√≥n: El sistema prueba m√∫ltiples URLs autom√°ticamente")
        
        print("\n   2. 'does not appear to be a valid module'")
        print("      ‚Ä¢ Causa: Modelo removido/movido en TensorFlow Hub")
        print("      ‚Ä¢ Soluci√≥n: Fallback autom√°tico a U-Net local")
        
        print("\n   3. Problemas de conectividad")
        print("      ‚Ä¢ Causa: Firewall, proxy o conexi√≥n limitada")
        print("      ‚Ä¢ Soluci√≥n: Usar modelos locales (U-Net con ResNet50)")
        
        print("\nüîß SOLUCIONES IMPLEMENTADAS:")
        print("   ‚úì M√∫ltiples URLs de fallback para DeepLabV3")
        print("   ‚úì U-Net con encoder ResNet50 preentrenado")
        print("   ‚úì U-Net simple como √∫ltimo fallback")
        print("   ‚úì Detecci√≥n autom√°tica de errores")
        
        print("\nüí° RECOMENDACIONES:")
        print("   1. Para uso offline: usar modelos 'vgg16' o 'resnet50'")
        print("   2. Para segmentaci√≥n: aceptar el fallback U-Net")
        print("   3. Para m√°ximo rendimiento: instalar tensorflow-hub")
        print("      pip install tensorflow-hub")
        
        print("\nüöÄ MODELOS SIEMPRE DISPONIBLES:")
        print("   ‚Ä¢ VGG16 (clasificaci√≥n)")
        print("   ‚Ä¢ ResNet50/ResNet101 (clasificaci√≥n)")
        print("   ‚Ä¢ U-Net con ResNet50 (segmentaci√≥n)")
        print("   ‚Ä¢ LeNet simple (clasificaci√≥n b√°sica)")
        
        print("="*70)
    
    def _crear_unet_resnet50(self):
        """
        Crea un modelo U-Net con encoder ResNet50 preentrenado.
        Fallback robusto cuando TensorFlow Hub no est√° disponible.
        """
        try:
            print("üîÑ Construyendo U-Net con ResNet50...")
            
            # Cargar ResNet50 preentrenado como encoder
            base_model = tf.keras.applications.ResNet50(
                input_shape=(224, 224, 3),
                include_top=False,
                weights='imagenet'
            )
            
            # Congelar el encoder para usar caracter√≠sticas preentrenadas
            base_model.trainable = False
            
            # Obtener capas intermedias para skip connections
            layer_outputs = {
                'conv1_relu': base_model.get_layer('conv1_relu').output,        # 112x112
                'conv2_block3_out': base_model.get_layer('conv2_block3_out').output,  # 56x56
                'conv3_block4_out': base_model.get_layer('conv3_block4_out').output,  # 28x28
                'conv4_block6_out': base_model.get_layer('conv4_block6_out').output,  # 14x14
                'conv5_block3_out': base_model.get_layer('conv5_block3_out').output,  # 7x7
            }
            
            inputs = base_model.input
            
            # Obtener caracter√≠sticas del encoder
            skip1 = layer_outputs['conv1_relu']
            skip2 = layer_outputs['conv2_block3_out']
            skip3 = layer_outputs['conv3_block4_out']
            skip4 = layer_outputs['conv4_block6_out']
            x = layer_outputs['conv5_block3_out']
            
            # Decoder con skip connections (estilo U-Net)
            # Bloque 1: 7x7 -> 14x14
            x = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(x)
            x = tf.keras.layers.Concatenate()([x, skip4])
            x = tf.keras.layers.Conv2D(512, 3, padding='same', activation='relu', 
                                      kernel_initializer='he_normal')(x)
            x = tf.keras.layers.BatchNormalization()(x)
            x = tf.keras.layers.Conv2D(512, 3, padding='same', activation='relu',
                                      kernel_initializer='he_normal')(x)
            x = tf.keras.layers.BatchNormalization()(x)
            
            # Bloque 2: 14x14 -> 28x28
            x = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(x)
            x = tf.keras.layers.Concatenate()([x, skip3])
            x = tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu',
                                      kernel_initializer='he_normal')(x)
            x = tf.keras.layers.BatchNormalization()(x)
            x = tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu',
                                      kernel_initializer='he_normal')(x)
            x = tf.keras.layers.BatchNormalization()(x)
            
            # Bloque 3: 28x28 -> 56x56
            x = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(x)
            x = tf.keras.layers.Concatenate()([x, skip2])
            x = tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu',
                                      kernel_initializer='he_normal')(x)
            x = tf.keras.layers.BatchNormalization()(x)
            x = tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu',
                                      kernel_initializer='he_normal')(x)
            x = tf.keras.layers.BatchNormalization()(x)
            
            # Bloque 4: 56x56 -> 112x112
            x = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(x)
            x = tf.keras.layers.Concatenate()([x, skip1])
            x = tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu',
                                      kernel_initializer='he_normal')(x)
            x = tf.keras.layers.BatchNormalization()(x)
            x = tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu',
                                      kernel_initializer='he_normal')(x)
            x = tf.keras.layers.BatchNormalization()(x)
            
            # Bloque 5: 112x112 -> 224x224
            x = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(x)
            x = tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu',
                                      kernel_initializer='he_normal')(x)
            x = tf.keras.layers.BatchNormalization()(x)
            
            # Capa de salida - segmentaci√≥n multiclase (21 clases PASCAL VOC + background)
            outputs = tf.keras.layers.Conv2D(21, 1, activation='softmax', 
                                           name='segmentation_output')(x)
            
            model = tf.keras.Model(inputs=inputs, outputs=outputs, name='unet_resnet50_segmentation')
            
            # Compilar modelo
            model.compile(
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy', tf.keras.metrics.SparseCategoricalAccuracy()]
            )
            
            print("‚úì U-Net con ResNet50 creado exitosamente")
            print("  Encoder: ResNet50 preentrenado (ImageNet) - Congelado")
            print("  Decoder: U-Net personalizado con skip connections") 
            print("  Salida: 21 clases (PASCAL VOC)")
            print("  Resoluci√≥n: 224x224")
            
            return model
            
        except Exception as e:
            print(f"‚ùå Error creando U-Net con ResNet50: {e}")
            # √öltimo fallback: U-Net simple
            return self._crear_unet_simple_fallback()
    
    def _crear_unet_simple_fallback(self):
        """
        Crea un U-Net simple desde cero como √∫ltimo fallback.
        """
        try:
            print("üîÑ Creando U-Net simple como fallback...")
            
            inputs = tf.keras.layers.Input(shape=(224, 224, 3))
            
            # Encoder (Contracting path)
            c1 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)
            c1 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c1)
            p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)
            
            c2 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)
            c2 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c2)
            p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)
            
            c3 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)
            c3 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c3)
            p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)
            
            c4 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(p3)
            c4 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c4)
            p4 = tf.keras.layers.MaxPooling2D((2, 2))(c4)
            
            # Bottom
            c5 = tf.keras.layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)
            c5 = tf.keras.layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)
            
            # Decoder (Expansive path)
            u6 = tf.keras.layers.UpSampling2D((2, 2))(c5)
            u6 = tf.keras.layers.Concatenate()([u6, c4])
            c6 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(u6)
            c6 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c6)
            
            u7 = tf.keras.layers.UpSampling2D((2, 2))(c6)
            u7 = tf.keras.layers.Concatenate()([u7, c3])
            c7 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u7)
            c7 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c7)
            
            u8 = tf.keras.layers.UpSampling2D((2, 2))(c7)
            u8 = tf.keras.layers.Concatenate()([u8, c2])
            c8 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u8)
            c8 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c8)
            
            u9 = tf.keras.layers.UpSampling2D((2, 2))(c8)
            u9 = tf.keras.layers.Concatenate()([u9, c1])
            c9 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u9)
            c9 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c9)
            
            # Output layer - segmentaci√≥n binaria simple
            outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)
            
            model = tf.keras.Model(inputs=inputs, outputs=outputs, name='unet_simple')
            
            model.compile(
                optimizer='adam',
                loss='binary_crossentropy',
                metrics=['accuracy']
            )
            
            print("‚úì U-Net simple creado como fallback")
            print("  Arquitectura: U-Net cl√°sica desde cero")
            print("  Salida: Segmentaci√≥n binaria")
            
            return model
            
        except Exception as e:
            print(f"‚ùå Error cr√≠tico creando U-Net simple: {e}")
            return None
            traceback.print_exc()
            return None
    
    def crear_mask_rcnn_simple(self):
        """
        Carga Mask R-CNN preentrenado usando detectron2 o TensorFlow Hub.
        Si no est√° disponible, usa un modelo de segmentaci√≥n de instancias preentrenado.
        """
        try:
            # Intentar usar detectron2 (como en el notebook)
            try:
                from detectron2 import model_zoo
                from detectron2.engine import DefaultPredictor
                from detectron2.config import get_cfg
                
                cfg = get_cfg()
                cfg.merge_from_file(
                    model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
                )
                cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(
                    "COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"
                )
                cfg.MODEL.ROI_HEADS.NUM_CLASSES = 80
                cfg.MODEL.DEVICE = "cpu"  # Forzar CPU
                cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5
                
                predictor = DefaultPredictor(cfg)
                print("‚úì Mask R-CNN cargado desde detectron2 (modelo COCO preentrenado)")
                return predictor
                
            except ImportError:
                print("‚ö†Ô∏è  detectron2 no disponible, usando alternativa con DeepLabV3+")
                
                # Alternativa: Usar DeepLabV3+ para segmentaci√≥n sem√°ntica
                # Este modelo est√° preentrenado en PASCAL VOC y puede detectar personas
                from tensorflow.keras.applications import ResNet50
                
                # Crear un modelo basado en DeepLab simplificado
                base_model = ResNet50(
                    weights='imagenet',
                    include_top=False,
                    input_shape=(224, 224, 3)
                )
                
                # Usar las capas intermedias para segmentaci√≥n
                inputs = base_model.input
                
                # Extractor de caracter√≠sticas en m√∫ltiples escalas
                layer_outputs = [
                    base_model.get_layer('conv2_block3_out').output,  # 56x56
                    base_model.get_layer('conv3_block4_out').output,  # 28x28
                    base_model.get_layer('conv4_block6_out').output,  # 14x14
                    base_model.get_layer('conv5_block3_out').output,  # 7x7
                ]
                
                # ASPP (Atrous Spatial Pyramid Pooling) - similar a DeepLab
                x = layer_outputs[-1]
                
                # Convoluciones con diferentes tasas de dilataci√≥n
                atrous_rates = [6, 12, 18]
                aspp_outputs = []
                
                # 1x1 convolution
                conv1x1 = tf.keras.layers.Conv2D(256, 1, padding='same', activation='relu')(x)
                aspp_outputs.append(conv1x1)
                
                # Convoluciones dilatadas (atrous convolutions)
                for rate in atrous_rates:
                    aspp_conv = tf.keras.layers.Conv2D(
                        256, 3, padding='same', dilation_rate=rate, activation='relu')(x)
                    aspp_outputs.append(aspp_conv)
                
                # Global average pooling
                gap = tf.keras.layers.GlobalAveragePooling2D()(x)
                gap = tf.keras.layers.Reshape((1, 1, x.shape[-1]))(gap)
                gap = tf.keras.layers.Conv2D(256, 1, activation='relu')(gap)
                gap = tf.keras.layers.UpSampling2D(size=(x.shape[1], x.shape[2]), interpolation='bilinear')(gap)
                aspp_outputs.append(gap)
                
                # Concatenar todas las caracter√≠sticas ASPP
                x = tf.keras.layers.Concatenate()(aspp_outputs)
                x = tf.keras.layers.Conv2D(256, 1, padding='same', activation='relu')(x)
                x = tf.keras.layers.Dropout(0.1)(x)
                
                # Decoder
                # Upsampling progresivo
                x = tf.keras.layers.UpSampling2D(size=(4, 4), interpolation='bilinear')(x)  # 28x28
                
                # Skip connection con capa intermedia
                skip = layer_outputs[1]  # 28x28
                skip = tf.keras.layers.Conv2D(48, 1, padding='same', activation='relu')(skip)
                x = tf.keras.layers.Concatenate()([x, skip])
                
                x = tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu')(x)
                x = tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu')(x)
                
                # Upsampling final a tama√±o original
                x = tf.keras.layers.UpSampling2D(size=(8, 8), interpolation='bilinear')(x)  # 224x224
                
                # Salida: m√°scara de segmentaci√≥n multiclase (21 clases de PASCAL VOC)
                # Clase 0: background, Clase 15: person (persona)
                outputs = tf.keras.layers.Conv2D(
                    21, 1, padding='same', activation='softmax', name='segmentation')(x)
                
                model = tf.keras.Model(inputs=inputs, outputs=outputs, name='deeplabv3plus')
                
                # Congelar el backbone
                for layer in base_model.layers:
                    layer.trainable = False
                
                model.compile(
                    optimizer='adam',
                    loss='sparse_categorical_crossentropy',
                    metrics=['accuracy']
                )
                
                print("‚úì DeepLabV3+ creado (alternativa a Mask R-CNN)")
                print("  Nota: Puede detectar 21 clases de PASCAL VOC incluyendo personas")
                return model
                
        except Exception as e:
            print(f"‚ùå Error creando Mask R-CNN: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def cargar_modelo(self, nombre_modelo):
        """
        Carga un modelo espec√≠fico.
        
        Args:
            nombre_modelo: Nombre del modelo a cargar
            
        Returns:
            bool: True si se carg√≥ exitosamente
        """
        try:
            print(f"Cargando modelo: {nombre_modelo}")
            
            if nombre_modelo == 'lenet':
                modelo = self.crear_lenet()
                self.configuracion['tama√±o_entrada'] = (32, 32)
                
            elif nombre_modelo == 'alexnet':
                modelo = self.crear_alexnet()
                self.configuracion['tama√±o_entrada'] = (224, 224)
                
            elif nombre_modelo == 'vgg16':
                modelo = self.cargar_modelo_vgg16()
                self.configuracion['tama√±o_entrada'] = (224, 224)
                
            elif nombre_modelo == 'resnet50':
                modelo = self.cargar_modelo_resnet50()
                self.configuracion['tama√±o_entrada'] = (224, 224)
                
            elif nombre_modelo == 'resnet101':
                modelo = self.cargar_modelo_resnet101()
                self.configuracion['tama√±o_entrada'] = (224, 224)
                
            elif nombre_modelo == 'yolo':
                modelo = self.cargar_modelo_yolo()
                if modelo is None:
                    return False
            
            elif nombre_modelo == 'yolo_custom':
                modelo = self.cargar_modelo_yolo_custom()
                if modelo is None:
                    return False
                
            elif nombre_modelo == 'ssd':
                modelo = self.cargar_modelo_ssd()
                if modelo is None:
                    return False
                self.configuracion['tama√±o_entrada'] = (224, 224)
                
            elif nombre_modelo == 'rcnn':
                modelo = self.cargar_modelo_rcnn()
                if modelo is None:
                    return False
                self.configuracion['tama√±o_entrada'] = (224, 224)
                
            elif nombre_modelo == 'unet':
                modelo = self.crear_unet_simple()
                self.configuracion['tama√±o_entrada'] = (224, 224)
                
            elif nombre_modelo == 'mask_rcnn':
                modelo = self.crear_mask_rcnn_simple()
                self.configuracion['tama√±o_entrada'] = (224, 224)
                
            else:
                print(f"Modelo {nombre_modelo} no implementado")
                return False
            
            self.modelos_cargados[nombre_modelo] = modelo
            self.modelo_activo = nombre_modelo
            print(f"Modelo {nombre_modelo} cargado exitosamente")
            return True
            
        except Exception as e:
            print(f"Error cargando modelo {nombre_modelo}: {e}")
            return False
    
    def preprocesar_frame(self, frame, modelo_nombre):
        """
        Preprocesa un frame para el modelo espec√≠fico.
        
        Args:
            frame: Frame de video (puede estar en RGB o BGR)
            modelo_nombre: Nombre del modelo
            
        Returns:
            Frame preprocesado
        """
        try:
            # Obtener tama√±o de entrada seg√∫n el modelo
            if modelo_nombre == 'lenet':
                tama√±o = (32, 32)
            else:
                tama√±o = (224, 224)
            
            # Redimensionar
            frame_resized = cv2.resize(frame, tama√±o)
            
            # Para modelos de ImageNet, usar el preprocesamiento espec√≠fico de Keras
            modelos_imagenet = ['vgg16', 'resnet50', 'resnet101', 'alexnet']  # AlexNet usa VGG16
            
            if modelo_nombre in modelos_imagenet:
                # IMPORTANTE: VGG16, AlexNet y ResNet50 esperan BGR (modo Caffe)
                #             ResNet V2 espera RGB (modo Torch)
                
                # Asumimos que frame_resized viene en RGB (convertido desde BGR)
                # VGG16, AlexNet y ResNet50 necesitan BGR
                if modelo_nombre in ['vgg16', 'resnet50', 'alexnet']:
                    # Convertir de RGB a BGR para estos modelos
                    frame_model = cv2.cvtColor(frame_resized, cv2.COLOR_RGB2BGR)
                else:
                    # ResNet101 V2 usa RGB
                    frame_model = frame_resized
                
                # Expandir dimensiones para batch
                frame_batch = np.expand_dims(frame_model, axis=0)
                
                # Aplicar preprocesamiento espec√≠fico de cada modelo
                # Estos m√©todos esperan valores en [0, 255]
                if modelo_nombre in ['vgg16', 'alexnet']:  # AlexNet usa preprocesamiento de VGG16
                    from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess
                    frame_prep = vgg_preprocess(frame_batch.copy())
                elif modelo_nombre == 'resnet50':
                    from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess
                    frame_prep = resnet_preprocess(frame_batch.copy())
                elif modelo_nombre == 'resnet101':
                    # ResNet101V2 usa un preprocesamiento diferente a ResNet V1
                    from tensorflow.keras.applications.resnet_v2 import preprocess_input as resnetv2_preprocess
                    frame_prep = resnetv2_preprocess(frame_batch.copy())
                
                return frame_prep
            else:
                # Para modelos custom (LeNet, AlexNet): normalizaci√≥n simple
                # Convertir a RGB si es necesario
                if len(frame_resized.shape) == 3 and frame_resized.shape[2] == 3:
                    frame_rgb = frame_resized
                else:
                    frame_rgb = frame_resized
                
                # Normalizar a [0, 1]
                frame_norm = frame_rgb.astype(np.float32) / 255.0
                
                # A√±adir dimensi√≥n de batch
                frame_batch = np.expand_dims(frame_norm, axis=0)
                
                return frame_batch
            
        except Exception as e:
            print(f"Error preprocesando frame: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def detectar_en_frame(self, frame, modelo_nombre):
        """
        Realiza detecci√≥n/clasificaci√≥n en un frame.
        
        Args:
            frame: Frame de video
            modelo_nombre: Nombre del modelo a usar
            
        Returns:
            Resultados de la detecci√≥n
        """
        try:
            # Verificar que el modelo est√© cargado
            if modelo_nombre not in self.modelos_cargados:
                print(f"‚ö†Ô∏è  Modelo {modelo_nombre} no est√° cargado. Cargando...")
                if not self.cargar_modelo(modelo_nombre):
                    print(f"‚ùå No se pudo cargar el modelo {modelo_nombre}")
                    return None
            
            modelo = self.modelos_cargados[modelo_nombre]
            print(f"üîç Detectando con modelo: {modelo_nombre}")
            
            if modelo_nombre == 'yolo':
                return self._detectar_yolo(frame, modelo)
            elif modelo_nombre == 'yolo_custom':
                return self._detectar_yolo_custom(frame, modelo)
            elif modelo_nombre == 'ssd':
                if modelo is not None:
                    return self._detectar_ssd(frame, modelo)
                return None
            elif modelo_nombre == 'rcnn':
                if modelo is not None:
                    return self._detectar_rcnn(frame, modelo)
                return None
            elif modelo_nombre == 'unet':
                return self._detectar_unet(frame, modelo)
            elif modelo_nombre == 'mask_rcnn':
                return self._detectar_mask_rcnn(frame, modelo)
            else:
                resultado = self._clasificar_frame(frame, modelo, modelo_nombre)
                if resultado:
                    print(f"‚úÖ Detecci√≥n exitosa: {resultado.get('clase', 'N/A')}")
                else:
                    print(f"‚ö†Ô∏è  No se obtuvieron resultados de clasificaci√≥n")
                return resultado
                
        except Exception as e:
            print(f"Error en detecci√≥n: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _clasificar_frame(self, frame, modelo, modelo_nombre):
        """Clasifica un frame usando modelos de clasificaci√≥n."""
        try:
            print(f"  üì∏ Preprocesando frame (shape: {frame.shape})...")
            # Preprocesar frame
            frame_prep = self.preprocesar_frame(frame, modelo_nombre)
            if frame_prep is None:
                print(f"  ‚ùå Error: preprocesamiento retorn√≥ None")
                return None
            
            print(f"  üß† Realizando predicci√≥n con {modelo_nombre}...")
            # Realizar predicci√≥n
            prediccion = modelo.predict(frame_prep, verbose=0)
            print(f"  ‚úÖ Predicci√≥n completada (shape: {prediccion.shape})")
            
            # Determinar si es modelo de ImageNet (VGG16, ResNet50, ResNet101, AlexNet) o custom (LeNet)
            modelos_imagenet = ['vgg16', 'resnet50', 'resnet101', 'alexnet']  # AlexNet ahora usa VGG16
            
            if modelo_nombre in modelos_imagenet:
                print(f"  üìä Decodificando predicciones de ImageNet...")
                # Para modelos de ImageNet: usar decode_predictions de Keras
                # Esto autom√°ticamente mapea los √≠ndices a las clases correctas de ImageNet
                clases_decodificadas = decode_predictions(prediccion, top=5)[0]
                
                # Crear lista de las top 5 mejores predicciones con los nombres correctos
                top_5_clases = []
                for (codigo_imagenet, nombre_clase, confianza) in clases_decodificadas:
                    top_5_clases.append({
                        'clase': nombre_clase.replace('_', ' ').title(),
                        'clase_original': nombre_clase,
                        'confianza': float(confianza),
                        'codigo': codigo_imagenet
                    })
                
                # La clase principal es la de mayor confianza
                clase_principal = top_5_clases[0]['clase']
                confianza_principal = top_5_clases[0]['confianza']
                
                # Determinar si hay sombreros en las predicciones (usar nombres originales de ImageNet)
                clases_sombrero = ['cowboy_hat', 'sombrero', 'pickelhaube', 'shower_cap', 'mortarboard', 'bonnet']
                clases_cabeza = ['crash_helmet', 'football_helmet', 'bathing_cap']
                
                # Verificar si alguna de las top 5 predicciones es un sombrero
                hay_sombrero = False
                mejor_sombrero = None
                for pred in top_5_clases:
                    nombre_original = pred['clase_original']
                    # Buscar sombreros exactos
                    if nombre_original in clases_sombrero:
                        hay_sombrero = True
                        if mejor_sombrero is None or pred['confianza'] > mejor_sombrero['confianza']:
                            mejor_sombrero = pred
                    # Buscar objetos relacionados con cabeza/protecci√≥n
                    elif nombre_original in clases_cabeza:
                        if not hay_sombrero and (mejor_sombrero is None or pred['confianza'] > mejor_sombrero['confianza']):
                            mejor_sombrero = pred
                    # Buscar por palabras clave relacionadas
                    elif any(palabra in nombre_original.lower() for palabra in ['hat', 'cap', 'helmet', 'hood', 'beret', 'turban']):
                        if not hay_sombrero and (mejor_sombrero is None or pred['confianza'] > mejor_sombrero['confianza']):
                            mejor_sombrero = pred
                
                resultado = {
                    'tipo': 'clasificacion_imagenet',
                    'clase': clase_principal,
                    'confianza': confianza_principal,
                    'top_5_clases': top_5_clases,  # Ahora devuelve top 5
                    'deteccion_sombrero': 'con_sombrero' if hay_sombrero else 'sin_sombrero',
                    'mejor_sombrero': mejor_sombrero,
                    'bbox': None,
                    'segmentacion': None
                }
                
            else:
                # Para modelos custom (LeNet, AlexNet): usar clasificaci√≥n binaria de sombreros
                clase_idx = np.argmax(prediccion[0])
                confianza = prediccion[0][clase_idx]
                
                resultado = {
                    'tipo': 'clasificacion',
                    'clase': self.clases_sombrero[clase_idx],
                    'confianza': float(confianza),
                    'bbox': None,
                    'segmentacion': None
                }
            
            return resultado
            
        except Exception as e:
            print(f"Error en clasificaci√≥n: {e}")
            return None
    
    def _detectar_yolo(self, frame, modelo):
        """Detecta objetos usando YOLO con detecci√≥n de regi√≥n de cabeza/sombrero."""
        try:
            results = modelo(frame, verbose=False)
            
            detecciones = []
            for r in results:
                boxes = r.boxes
                if boxes is not None:
                    for box in boxes:
                        cls_id = int(box.cls)
                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                        conf = box.conf[0].cpu().numpy()
                        
                        # Detectar personas (clase 0 en COCO)
                        if cls_id == 0:
                            # Analizar regi√≥n de cabeza (20% superior del bbox)
                            altura_bbox = y2 - y1
                            y_cabeza = int(y1 + altura_bbox * 0.2)
                            
                            # Extraer regi√≥n de cabeza
                            region_cabeza = frame[int(y1):y_cabeza, int(x1):int(x2)]
                            
                            # Detectar sombrero por an√°lisis de color en regi√≥n superior
                            tiene_sombrero = self._detectar_sombrero_en_region(region_cabeza)
                            
                            clase = 'con_sombrero' if tiene_sombrero else 'persona'
                            
                            deteccion = {
                                'tipo': 'deteccion',
                                'clase': clase,
                                'confianza': float(conf),
                                'bbox': [int(x1), int(y1), int(x2), int(y2)],
                                'region_sombrero': [int(x1), int(y1), int(x2), y_cabeza] if tiene_sombrero else None,
                                'segmentacion': None
                            }
                            detecciones.append(deteccion)
                        
                        # Detectar otros objetos relevantes (sombreros sin persona)
                        # Clase 25 = backpack, 26 = umbrella, 27 = handbag (similar forma)
                        elif cls_id in [25, 26, 27, 31]:  # 31 = handbag
                            # Verificar si est√° en parte superior del frame (posible sombrero)
                            altura_frame = frame.shape[0]
                            if y1 < altura_frame * 0.4:
                                deteccion = {
                                    'tipo': 'deteccion',
                                    'clase': 'posible_sombrero',
                                    'confianza': float(conf),
                                    'bbox': [int(x1), int(y1), int(x2), int(y2)],
                                    'segmentacion': None
                                }
                                detecciones.append(deteccion)
            
            return detecciones if detecciones else None
            
        except Exception as e:
            print(f"Error en YOLO: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _detectar_sombrero_en_region(self, region):
        """Detecta si hay un sombrero en la regi√≥n de cabeza."""
        try:
            if region.size == 0 or region.shape[0] < 5 or region.shape[1] < 5:
                return False
            
            # Convertir a HSV para an√°lisis de color
            hsv = cv2.cvtColor(region, cv2.COLOR_BGR2HSV)
            
            # Detectar colores comunes de sombreros
            # Negro/Marr√≥n oscuro
            lower_dark = np.array([0, 0, 0])
            upper_dark = np.array([180, 255, 80])
            mask_dark = cv2.inRange(hsv, lower_dark, upper_dark)
            
            # Beige/Marr√≥n claro
            lower_beige = np.array([10, 20, 80])
            upper_beige = np.array([30, 150, 255])
            mask_beige = cv2.inRange(hsv, lower_beige, upper_beige)
            
            # Combinar m√°scaras
            mask_total = cv2.bitwise_or(mask_dark, mask_beige)
            
            # Si m√°s del 30% de la regi√≥n tiene color de sombrero
            porcentaje = (np.sum(mask_total > 0) / mask_total.size) * 100
            
            return porcentaje > 30
            
        except Exception as e:
            return False
    
    def _detectar_yolo_custom(self, frame, modelo):
        """
        Detecta sombreros usando YOLO Custom entrenado con Open Images.
        
        Args:
            frame: Frame de video en formato BGR (OpenCV)
            modelo: Modelo YOLO Custom cargado
            
        Returns:
            Lista de detecciones con formato:
            {
                'tipo': 'deteccion',
                'clase': nombre_clase,
                'confianza': float,
                'bbox': [x1, y1, x2, y2],
                'segmentacion': None
            }
        """
        try:
            # Ejecutar inferencia
            results = modelo(frame, verbose=False)
            
            detecciones = []
            for r in results:
                boxes = r.boxes
                if boxes is not None:
                    for box in boxes:
                        cls_id = int(box.cls)
                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                        conf = box.conf[0].cpu().numpy()
                        
                        # Obtener nombre de la clase del modelo
                        # Las clases personalizadas del modelo est√°n en modelo.names
                        clase_nombre = modelo.names[cls_id] if cls_id < len(modelo.names) else f'clase_{cls_id}'
                        
                        deteccion = {
                            'tipo': 'deteccion',
                            'clase': clase_nombre,
                            'confianza': float(conf),
                            'bbox': [int(x1), int(y1), int(x2), int(y2)],
                            'segmentacion': None,
                            'clase_id': cls_id
                        }
                        detecciones.append(deteccion)
            
            return detecciones if detecciones else None
            
        except Exception as e:
            print(f"Error en YOLO Custom: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _detectar_ssd(self, frame, modelo):
        """Detecta objetos usando SSD MobileNet."""
        try:
            # Convertir frame a RGB si es necesario
            if len(frame.shape) == 3 and frame.shape[2] == 3:
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            else:
                frame_rgb = frame
            
            # Ejecutar detecci√≥n
            result = modelo(frame_rgb)
            
            detecciones = []
            
            # Si es el modelo de TF Hub
            if isinstance(result, dict) and 'detection_boxes' in result:
                boxes = result['detection_boxes'][0].numpy()
                classes = result['detection_classes'][0].numpy().astype(int)
                scores = result['detection_scores'][0].numpy()
                
                altura, ancho = frame.shape[:2]
                
                for i, score in enumerate(scores):
                    if score > self.configuracion['umbral_confianza']:
                        cls_id = classes[i]
                        
                        # Clase 1 en COCO = persona
                        if cls_id == 1:
                            ymin, xmin, ymax, xmax = boxes[i]
                            x1 = int(xmin * ancho)
                            y1 = int(ymin * altura)
                            x2 = int(xmax * ancho)
                            y2 = int(ymax * altura)
                            
                            # Analizar regi√≥n de cabeza
                            altura_bbox = y2 - y1
                            y_cabeza = int(y1 + altura_bbox * 0.2)
                            region_cabeza = frame[y1:y_cabeza, x1:x2]
                            
                            tiene_sombrero = self._detectar_sombrero_en_region(region_cabeza)
                            
                            deteccion = {
                                'tipo': 'deteccion',
                                'clase': 'con_sombrero' if tiene_sombrero else 'persona',
                                'confianza': float(score),
                                'bbox': [x1, y1, x2, y2],
                                'region_sombrero': [x1, y1, x2, y_cabeza] if tiene_sombrero else None,
                                'segmentacion': None
                            }
                            detecciones.append(deteccion)
            
            # Si es el modelo simplificado
            elif isinstance(result, dict) and 'features' in result:
                # Para el modelo simplificado, retornar detecci√≥n gen√©rica
                deteccion = {
                    'tipo': 'clasificacion',
                    'clase': 'deteccion_simple',
                    'confianza': 0.5,
                    'bbox': None,
                    'segmentacion': None,
                    'mensaje': 'SSD simplificado - an√°lisis de features'
                }
                detecciones.append(deteccion)
            
            return detecciones if detecciones else None
            
        except Exception as e:
            print(f"Error en SSD: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _detectar_rcnn(self, frame, modelo):
        """Detecta objetos usando Faster R-CNN."""
        try:
            # Convertir frame a RGB si es necesario
            if len(frame.shape) == 3 and frame.shape[2] == 3:
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            else:
                frame_rgb = frame
            
            # Ejecutar detecci√≥n
            result = modelo(frame_rgb)
            
            detecciones = []
            
            # Si es el modelo de TF Hub
            if isinstance(result, dict) and 'detection_boxes' in result:
                boxes = result['detection_boxes'][0].numpy()
                classes = result['detection_classes'][0].numpy().astype(int)
                scores = result['detection_scores'][0].numpy()
                
                altura, ancho = frame.shape[:2]
                
                for i, score in enumerate(scores):
                    if score > self.configuracion['umbral_confianza']:
                        cls_id = classes[i]
                        
                        # Clase 1 en COCO = persona
                        if cls_id == 1:
                            ymin, xmin, ymax, xmax = boxes[i]
                            x1 = int(xmin * ancho)
                            y1 = int(ymin * altura)
                            x2 = int(xmax * ancho)
                            y2 = int(ymax * altura)
                            
                            # Analizar regi√≥n de cabeza
                            altura_bbox = y2 - y1
                            y_cabeza = int(y1 + altura_bbox * 0.2)
                            region_cabeza = frame[y1:y_cabeza, x1:x2]
                            
                            tiene_sombrero = self._detectar_sombrero_en_region(region_cabeza)
                            
                            deteccion = {
                                'tipo': 'deteccion',
                                'clase': 'con_sombrero' if tiene_sombrero else 'persona',
                                'confianza': float(score),
                                'bbox': [x1, y1, x2, y2],
                                'region_sombrero': [x1, y1, x2, y_cabeza] if tiene_sombrero else None,
                                'segmentacion': None
                            }
                            detecciones.append(deteccion)
            
            # Si es el modelo simplificado
            elif isinstance(result, dict) and 'features' in result:
                # Para el modelo simplificado, retornar detecci√≥n gen√©rica
                deteccion = {
                    'tipo': 'clasificacion',
                    'clase': 'deteccion_simple',
                    'confianza': 0.5,
                    'bbox': None,
                    'segmentacion': None,
                    'mensaje': 'R-CNN simplificado - an√°lisis de features'
                }
                detecciones.append(deteccion)
            
            return detecciones if detecciones else None
            
        except Exception as e:
            print(f"Error en Faster R-CNN: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _detectar_unet(self, frame, modelo):
        """
        Realiza segmentaci√≥n usando U-Net con encoder preentrenado.
        Genera una m√°scara binaria de segmentaci√≥n.
        """
        try:
            print(f"   üîç Iniciando segmentaci√≥n U-Net...")
            print(f"   Frame shape: {frame.shape}")
            
            # Obtener dimensiones del frame original
            altura_original, ancho_original = frame.shape[:2]
            
            # Preprocesar frame para U-Net
            frame_prep = self.preprocesar_frame(frame, 'unet')
            if frame_prep is None:
                print(f"   ‚ùå Error en preprocesamiento")
                return None
            
            print(f"   Frame preprocesado shape: {frame_prep.shape}")
            
            # Realizar predicci√≥n de segmentaci√≥n
            print(f"   üîÆ Ejecutando predicci√≥n...")
            mascara_pred = modelo.predict(frame_prep, verbose=0)
            print(f"   Predicci√≥n shape: {mascara_pred.shape}")
            print(f"   Predicci√≥n tipo: {type(mascara_pred)}")
            print(f"   Predicci√≥n rango: [{mascara_pred.min():.3f}, {mascara_pred.max():.3f}]")
            
            # Verificar el formato de la salida
            if isinstance(mascara_pred, dict):
                # Es TensorFlow Hub (DeepLabV3)
                print(f"   üìã Formato: Diccionario TensorFlow Hub")
                mascara_224 = mascara_pred  # Ya viene como array de clases
            elif len(mascara_pred.shape) == 4 and mascara_pred.shape[-1] == 21:
                # Es U-Net multiclase (21 clases PASCAL VOC)
                print(f"   üìã Formato: U-Net multiclase (21 clases)")
                # Obtener la clase con mayor probabilidad para cada pixel
                mascara_clases = np.argmax(mascara_pred[0], axis=-1)
                # Crear m√°scara binaria (todo lo que no sea background = clase 0)
                mascara_224 = (mascara_clases > 0).astype(np.uint8)
                print(f"   Clases detectadas: {np.unique(mascara_clases)}")
            elif len(mascara_pred.shape) == 4 and mascara_pred.shape[-1] == 1:
                # Es U-Net con salida sigmoid (m√°scara binaria)
                print(f"   üìã Formato: U-Net binario (sigmoid)")
                mascara_224 = (mascara_pred[0, :, :, 0] > 0.5).astype(np.uint8)
            elif len(mascara_pred.shape) == 3:
                # Es TensorFlow Hub sin batch dimension
                print(f"   üìã Formato: Sin batch dimension")
                mascara_224 = (mascara_pred > 0).astype(np.uint8)
            else:
                # Formato inesperado
                print(f"   ‚ö†Ô∏è  Formato de m√°scara inesperado: {mascara_pred.shape}")
                mascara_224 = np.zeros((224, 224), dtype=np.uint8)
            
            print(f"   M√°scara procesada shape: {mascara_224.shape}")
            print(f"   Valores √∫nicos en m√°scara: {np.unique(mascara_224)}")
            pixels_activos = np.sum(mascara_224 > 0)
            print(f"   P√≠xeles activos: {pixels_activos} de {mascara_224.size}")
            
            # Redimensionar m√°scara al tama√±o del frame original
            mascara = cv2.resize(
                mascara_224, 
                (ancho_original, altura_original), 
                interpolation=cv2.INTER_NEAREST
            )
            
            # Convertir a binario si no lo es
            if mascara.dtype != np.uint8:
                mascara = (mascara > 0).astype(np.uint8)
            
            # Encontrar contornos en la m√°scara
            contornos, _ = cv2.findContours(
                mascara, 
                cv2.RETR_EXTERNAL, 
                cv2.CHAIN_APPROX_SIMPLE
            )
            
            # Calcular estad√≠sticas de segmentaci√≥n
            area_segmentada = np.sum(mascara > 0)
            total_pixeles = mascara.shape[0] * mascara.shape[1]
            porcentaje_segmentado = area_segmentada / total_pixeles if total_pixeles > 0 else 0
            
            # Encontrar el contorno m√°s grande (objeto principal)
            objeto_detectado = False
            bbox = None
            area_mayor = 0
            
            if contornos:
                # Ordenar contornos por √°rea
                contornos = sorted(contornos, key=cv2.contourArea, reverse=True)
                
                # Tomar el contorno m√°s grande si es significativo
                for contorno in contornos:
                    area = cv2.contourArea(contorno)
                    if area > 100:  # √Årea m√≠nima para considerar
                        x, y, w, h = cv2.boundingRect(contorno)
                        bbox = [int(x), int(y), int(w), int(h)]
                        area_mayor = area
                        objeto_detectado = True
                        break
            
            # Determinar clase y confianza basada en m√©tricas robustas
            if objeto_detectado and porcentaje_segmentado > 0.005:  # 0.5% m√≠nimo
                # Calcular m√©tricas de calidad del contorno
                if bbox:
                    bbox_area = bbox[2] * bbox[3]
                    compacidad = area_mayor / bbox_area if bbox_area > 0 else 0
                    aspect_ratio = bbox[2] / bbox[3] if bbox[3] > 0 else 1
                    
                    # Clasificar seg√∫n caracter√≠sticas
                    if porcentaje_segmentado > 0.3:  # Objeto grande (>30%)
                        clase = 'objeto_grande'
                        confianza = min(0.7 + compacidad * 0.3, 0.98)
                    elif porcentaje_segmentado > 0.1:  # Objeto mediano (10-30%)
                        clase = 'objeto_mediano'
                        confianza = min(0.6 + compacidad * 0.3, 0.95)
                    else:  # Objeto peque√±o (0.5-10%)
                        clase = 'objeto_peque√±o'
                        confianza = min(0.4 + compacidad * 0.4, 0.90)
                else:
                    clase = 'region_segmentada'
                    confianza = min(porcentaje_segmentado * 10, 0.85)
            else:
                clase = 'sin_objeto'
                confianza = max(0.1, 1.0 - porcentaje_segmentado * 50)
            
            # Calcular m√©tricas adicionales
            num_objetos_significativos = len([c for c in contornos if cv2.contourArea(c) > 100]) if contornos else 0
            area_bbox = (bbox[2] * bbox[3]) if bbox else 0
            densidad = (area_mayor / area_bbox * 100) if area_bbox > 0 else 0
            
            print(f"   üìä Resultados finales:")
            print(f"   - Clase determinada: {clase}")
            print(f"   - Confianza: {confianza:.3f}")
            print(f"   - √Årea segmentada: {area_segmentada} p√≠xeles")
            print(f"   - Porcentaje: {porcentaje_segmentado*100:.2f}%")
            print(f"   - Objetos significativos: {num_objetos_significativos}")
            print(f"   - BBox: {bbox}")
            
            resultado = {
                'tipo': 'segmentacion_unet',
                'clase': clase,
                'confianza': float(confianza),
                'bbox': bbox,
                'segmentacion': mascara,
                'area_segmentada': int(area_segmentada),
                'porcentaje': float(porcentaje_segmentado * 100),
                'num_objetos': num_objetos_significativos,
                'metricas': {
                    'area_contorno_principal': int(area_mayor),
                    'area_bbox': int(area_bbox),
                    'densidad': float(densidad),
                    'pixeles_totales': int(total_pixeles)
                }
            }
            
            print(f"   ‚úÖ Segmentaci√≥n U-Net completada exitosamente")
            return resultado
            
        except Exception as e:
            print(f"‚ùå Error en U-Net: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _detectar_mask_rcnn(self, frame, modelo):
        """
        Realiza segmentaci√≥n de instancias usando Mask R-CNN (detectron2) 
        o DeepLabV3+ (TensorFlow).
        """
        try:
            print(f"   üîç Iniciando Mask R-CNN...")
            print(f"   Frame shape: {frame.shape}")
            print(f"   Tipo de modelo: {type(modelo)}")
            
            # Verificar si es un predictor de detectron2 o modelo de TensorFlow
            if hasattr(modelo, 'model'):  # Es un DefaultPredictor de detectron2
                print(f"   üìã Usando detectron2")
                return self._detectar_mask_rcnn_detectron2(frame, modelo)
            else:  # Es un modelo de TensorFlow (DeepLabV3+)
                print(f"   üìã Usando modelo TensorFlow alternativo")
                return self._detectar_deeplabv3(frame, modelo)
                
        except Exception as e:
            print(f"‚ùå Error en Mask R-CNN/DeepLab: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _detectar_mask_rcnn_detectron2(self, frame, predictor):
        """Detecta usando Mask R-CNN real (detectron2)."""
        try:
            print(f"   üîÆ Ejecutando predicci√≥n detectron2...")
            
            # Convertir de RGB a BGR para detectron2
            frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
            print(f"   Frame BGR shape: {frame_bgr.shape}")
            
            # Realizar predicci√≥n
            outputs = predictor(frame_bgr)
            print(f"   Predicci√≥n completada")
            
            # Extraer instancias detectadas
            instances = outputs["instances"].to("cpu")
            num_detecciones = len(instances)
            print(f"   Instancias detectadas: {num_detecciones}")
            
            if num_detecciones == 0:
                print(f"   ‚ö†Ô∏è  No se detectaron objetos")
                return {
                    'tipo': 'segmentacion_mask_rcnn',
                    'clase': 'sin_objetos',
                    'confianza': 0.0,
                    'num_instancias': 0,
                    'instancias': []
                }
            
            # Procesar cada instancia detectada
            instancias_detectadas = []
            
            for i in range(num_detecciones):
                bbox = instances.pred_boxes[i].tensor.numpy()[0]
                clase_id = instances.pred_classes[i].item()
                score = instances.scores[i].item()
                mascara = instances.pred_masks[i].numpy()
                
                print(f"   Instancia {i+1}: clase_id={clase_id}, score={score:.3f}, bbox_shape={bbox.shape}")
                
                instancias_detectadas.append({
                    'bbox': bbox.tolist(),
                    'clase_id': int(clase_id),
                    'clase': f"clase_{clase_id}",  # Agregar nombre de clase
                    'confianza': float(score),
                    'mascara': mascara
                })
            
            # Tomar la instancia con mayor confianza
            mejor_inst = max(instancias_detectadas, key=lambda x: x['confianza'])
            print(f"   Mejor instancia: {mejor_inst['clase']} con confianza {mejor_inst['confianza']:.3f}")
            
            resultado = {
                'tipo': 'segmentacion_mask_rcnn',
                'clase': mejor_inst['clase'],
                'confianza': mejor_inst['confianza'],
                'bbox': mejor_inst['bbox'],
                'segmentacion': mejor_inst['mascara'],
                'num_instancias': len(instancias_detectadas),
                'instancias': instancias_detectadas
            }
            
            print(f"   ‚úÖ Mask R-CNN (detectron2) completado exitosamente")
            return resultado
            
        except Exception as e:
            print(f"‚ùå Error en Mask R-CNN (detectron2): {e}")
            return None
    
    def _detectar_deeplabv3(self, frame, modelo):
        """Detecta usando DeepLabV3+ (segmentaci√≥n sem√°ntica)."""
        try:
            altura_original, ancho_original = frame.shape[:2]
            
            # Preprocesar frame
            frame_prep = self.preprocesar_frame(frame, 'mask_rcnn')
            if frame_prep is None:
                return None
            
            # Realizar predicci√≥n (21 clases de PASCAL VOC)
            prediccion = modelo.predict(frame_prep, verbose=0)
            
            # Obtener la m√°scara de clases (argmax)
            mascara_clases = np.argmax(prediccion[0], axis=-1).astype(np.uint8)
            
            # Redimensionar al tama√±o original
            mascara_resized = cv2.resize(
                mascara_clases, 
                (ancho_original, altura_original), 
                interpolation=cv2.INTER_NEAREST
            )
            
            # Clases PASCAL VOC: 0=background, 15=person
            # Buscar clases detectadas (excepto background)
            clases_unicas = np.unique(mascara_resized)
            clases_detectadas = [c for c in clases_unicas if c != 0]
            
            if len(clases_detectadas) == 0:
                return {
                    'tipo': 'segmentacion_semantica',
                    'clase': 'sin_objetos',
                    'confianza': 0.0,
                    'segmentacion': mascara_resized
                }
            
            # Crear m√°scara binaria combinada (todos los objetos)
            mascara_objetos = (mascara_resized > 0).astype(np.uint8)
            
            # Encontrar contornos
            contornos, _ = cv2.findContours(
                mascara_objetos, 
                cv2.RETR_EXTERNAL, 
                cv2.CHAIN_APPROX_SIMPLE
            )
            
            # Obtener bbox del contorno principal
            bbox = None
            if contornos:
                contorno_mayor = max(contornos, key=cv2.contourArea)
                x, y, w, h = cv2.boundingRect(contorno_mayor)
                bbox = [int(x), int(y), int(w), int(h)]
            
            # Calcular estad√≠sticas
            area_segmentada = np.sum(mascara_objetos)
            total_pixeles = mascara_objetos.shape[0] * mascara_objetos.shape[1]
            porcentaje = area_segmentada / total_pixeles if total_pixeles > 0 else 0
            
            # Nombres de clases PASCAL VOC (simplificado)
            nombres_clases = [
                'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',
                'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog',
                'horse', 'motorbike', 'person', 'pottedplant', 'sheep',
                'sofa', 'train', 'tvmonitor'
            ]
            
            # Clase principal detectada
            clase_principal = clases_detectadas[0]
            nombre_clase = nombres_clases[clase_principal] if clase_principal < len(nombres_clases) else f"clase_{clase_principal}"
            
            # Confianza basada en √°rea y n√∫mero de p√≠xeles
            confianza = min(porcentaje * 10, 0.95)
            
            resultado = {
                'tipo': 'segmentacion_semantica',
                'clase': nombre_clase,
                'confianza': float(confianza),
                'bbox': bbox,
                'segmentacion': mascara_resized,
                'area_segmentada': int(area_segmentada),
                'porcentaje': float(porcentaje * 100),
                'clases_detectadas': [nombres_clases[c] if c < len(nombres_clases) else f"clase_{c}" for c in clases_detectadas]
            }
            
            return resultado
            
        except Exception as e:
            print(f"‚ùå Error en DeepLabV3+: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def dibujar_detecciones(self, frame, detecciones):
        """
        Dibuja las detecciones en el frame.
        
        Args:
            frame: Frame original
            detecciones: Resultados de detecci√≥n
            
        Returns:
            Frame con detecciones dibujadas
        """
        if detecciones is None:
            return frame
        
        frame_result = frame.copy()
        
        # Si es una lista de detecciones (YOLO)
        if isinstance(detecciones, list):
            for det in detecciones:
                self._dibujar_deteccion_individual(frame_result, det)
        else:
            # Detecci√≥n individual (clasificaci√≥n o segmentaci√≥n)
            self._dibujar_deteccion_individual(frame_result, detecciones)
        
        return frame_result
    
    def _dibujar_deteccion_individual(self, frame, deteccion):
        """Dibuja una detecci√≥n individual."""
        try:
            tipo = deteccion['tipo']
            clase = deteccion['clase']
            confianza = deteccion['confianza']
            
            # Colores seg√∫n clase
            if clase == 'con_sombrero':
                color = (0, 255, 0)  # Verde
            else:
                color = (0, 0, 255)  # Rojo
            
            if tipo == 'deteccion' and deteccion['bbox']:
                # Dibujar bounding box
                x1, y1, x2, y2 = deteccion['bbox']
                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                
                # Texto con clase y confianza
                texto = f"{clase}: {confianza:.2f}"
                cv2.putText(frame, texto, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 
                           0.6, color, 2)
            
            elif tipo == 'segmentacion_unet':
                # Dibujar m√°scara de segmentaci√≥n U-Net
                mascara = deteccion.get('segmentacion')
                bbox = deteccion.get('bbox')
                
                if mascara is not None:
                    # Asegurar que la m√°scara tenga las mismas dimensiones que el frame
                    if mascara.shape[:2] != frame.shape[:2]:
                        altura_frame, ancho_frame = frame.shape[:2]
                        mascara = cv2.resize(mascara, (ancho_frame, altura_frame), interpolation=cv2.INTER_NEAREST)
                    
                    # Crear m√°scara coloreada seg√∫n la clase
                    mascara_color = np.zeros_like(frame)
                    if 'grande' in clase:
                        mascara_color[:, :, 1] = mascara * 255  # Verde para objetos grandes
                    elif 'mediano' in clase:
                        mascara_color[:, :, 0] = mascara * 255  # Azul para objetos medianos
                    else:
                        mascara_color[:, :, 2] = mascara * 255  # Rojo para objetos peque√±os o otros
                    
                    # Superponer m√°scara con transparencia
                    alpha = 0.4
                    frame_overlay = cv2.addWeighted(frame, 1-alpha, mascara_color, alpha, 0)
                    frame[:] = frame_overlay
                
                # Dibujar bounding box si existe
                if bbox:
                    x, y, w, h = bbox
                    cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)
                
                # Informaci√≥n de segmentaci√≥n
                porcentaje = deteccion.get('porcentaje', 0)
                area = deteccion.get('area_segmentada', 0)
                num_objetos = deteccion.get('num_objetos', 0)
                
                texto1 = f"U-Net: {clase}"
                texto2 = f"Conf: {confianza:.2f} | Area: {porcentaje:.1f}%"
                texto3 = f"Objetos: {num_objetos} | Pixeles: {area}"
                
                cv2.putText(frame, texto1, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 
                           0.7, color, 2)
                cv2.putText(frame, texto2, (10, 55), cv2.FONT_HERSHEY_SIMPLEX, 
                           0.5, color, 2)
                cv2.putText(frame, texto3, (10, 75), cv2.FONT_HERSHEY_SIMPLEX, 
                           0.4, color, 1)
                           
            elif tipo == 'segmentacion_mask_rcnn':
                # Mask R-CNN con m√∫ltiples instancias
                instancias = deteccion.get('instancias', [])
                
                # Dibujar cada instancia
                for i, inst in enumerate(instancias):
                    mascara_inst = inst.get('mascara')
                    bbox_inst = inst.get('bbox')
                    clase_inst = inst.get('clase', f'objeto_{i}')
                    conf_inst = inst.get('confianza', 0)
                    
                    if mascara_inst is not None:
                        # Redimensionar si es necesario
                        if mascara_inst.shape[:2] != frame.shape[:2]:
                            altura_frame, ancho_frame = frame.shape[:2]
                            mascara_inst = cv2.resize(mascara_inst, (ancho_frame, altura_frame), 
                                                    interpolation=cv2.INTER_NEAREST)
                        
                        # Color diferente para cada instancia
                        colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255)]
                        color_inst = colors[i % len(colors)]
                        
                        # Crear m√°scara coloreada
                        mascara_color = np.zeros_like(frame)
                        for c in range(3):
                            mascara_color[:, :, c] = mascara_inst * color_inst[c]
                        
                        # Superponer
                        alpha = 0.3
                        frame_overlay = cv2.addWeighted(frame, 1-alpha, mascara_color, alpha, 0)
                        frame[:] = frame_overlay
                    
                    # Bounding box para la instancia
                    if bbox_inst:
                        x1, y1, x2, y2 = bbox_inst
                        cv2.rectangle(frame, (x1, y1), (x2, y2), color_inst, 2)
                        
                        # Etiqueta de la instancia
                        texto_inst = f"{clase_inst}: {conf_inst:.2f}"
                        cv2.putText(frame, texto_inst, (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 
                                   0.5, color_inst, 2)
                
                # Informaci√≥n general
                texto_general = f"Mask R-CNN: {len(instancias)} instancias"
                cv2.putText(frame, texto_general, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 
                           0.7, color, 2)
            
            elif tipo == 'segmentacion' and deteccion['segmentacion'] is not None:
                # Compatibilidad con formato anterior
                mascara = deteccion['segmentacion']
                
                # Asegurar que la m√°scara tenga las mismas dimensiones que el frame
                if mascara.shape[:2] != frame.shape[:2]:
                    altura_frame, ancho_frame = frame.shape[:2]
                    mascara = cv2.resize(mascara, (ancho_frame, altura_frame), interpolation=cv2.INTER_NEAREST)
                
                # Crear m√°scara coloreada
                mascara_color = np.zeros_like(frame)
                mascara_color[:, :, 1] = mascara * 255  # Canal verde
                
                # Superponer m√°scara
                frame_overlay = cv2.addWeighted(frame, 0.7, mascara_color, 0.3, 0)
                frame[:] = frame_overlay
                
                # Texto con informaci√≥n
                texto = f"Segmentacion {clase}: {confianza:.2f}"
                cv2.putText(frame, texto, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 
                           0.7, color, 2)
                           
            elif tipo == 'deteccion_segmentacion' and deteccion['segmentacion'] is not None:
                # Dibujar detecci√≥n + segmentaci√≥n (Mask R-CNN)
                mascara = deteccion['segmentacion']
                
                # Asegurar que la m√°scara tenga las mismas dimensiones que el frame
                if mascara.shape[:2] != frame.shape[:2]:
                    altura_frame, ancho_frame = frame.shape[:2]
                    mascara = cv2.resize(mascara, (ancho_frame, altura_frame), interpolation=cv2.INTER_NEAREST)
                
                # Crear m√°scara coloreada con transparencia
                mascara_color = np.zeros_like(frame)
                if clase == 'con_sombrero':
                    mascara_color[:, :, 1] = mascara * 255  # Verde para sombrero
                else:
                    mascara_color[:, :, 2] = mascara * 255  # Rojo para no-sombrero
                
                # Superponer m√°scara
                frame_overlay = cv2.addWeighted(frame, 0.8, mascara_color, 0.4, 0)
                frame[:] = frame_overlay
                
                # Texto con informaci√≥n detallada
                conf_cls = deteccion.get('confianza_clasificacion', confianza)
                porcentaje = deteccion.get('porcentaje_segmentado', 0) * 100
                texto1 = f"Mask R-CNN: {clase}"
                texto2 = f"Conf: {confianza:.2f} | Seg: {porcentaje:.1f}%"
                
                cv2.putText(frame, texto1, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 
                           0.7, color, 2)
                cv2.putText(frame, texto2, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 
                           0.5, color, 2)
                           
            elif tipo == 'clasificacion_imagenet':
                # Clasificaci√≥n con ImageNet - mostrar top 5 con nombres reales de decode_predictions
                top_5_clases = deteccion.get('top_5_clases', [])
                
                # Mostrar las 5 clases principales con formato mejorado
                y_offset = 30
                for i, pred in enumerate(top_5_clases[:5]):
                    # El nombre ya viene formateado de decode_predictions
                    nombre_clase = pred['clase']
                    texto = f"{i+1}. {nombre_clase}: {pred['confianza']:.3f}"
                    color_texto = (0, 255, 0) if i == 0 else (255, 255, 255)  # Verde para la primera, blanco para las dem√°s
                    cv2.putText(frame, texto, (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 
                               0.6, color_texto, 2)
                    y_offset += 25
                
                # Mostrar detecci√≥n de sombrero si aplica
                deteccion_sombrero = deteccion.get('deteccion_sombrero', 'sin_sombrero')
                mejor_sombrero = deteccion.get('mejor_sombrero')
                
                if deteccion_sombrero == 'con_sombrero' and mejor_sombrero:
                    nombre_sombrero = mejor_sombrero['clase']
                    texto_sombrero = f"SOMBRERO: {nombre_sombrero} ({mejor_sombrero['confianza']:.3f})"
                    cv2.putText(frame, texto_sombrero, (10, y_offset + 10), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                elif deteccion_sombrero == 'con_sombrero':
                    cv2.putText(frame, "SOMBRERO DETECTADO!", (10, y_offset + 10), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            
            else:
                # Clasificaci√≥n simple - texto en esquina
                texto = f"Clasificacion: {clase} ({confianza:.2f})"
                cv2.putText(frame, texto, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 
                           0.7, color, 2)
                
                # Dibujar c√≠rculo seg√∫n clasificaci√≥n
                centro = (frame.shape[1] - 50, 50)
                cv2.circle(frame, centro, 20, color, -1)
                
        except Exception as e:
            print(f"Error dibujando detecci√≥n: {e}")
    
    def procesar_video_tiempo_real(self, fuente=0):
        """
        Procesa video en tiempo real desde c√°mara o archivo.
        
        Args:
            fuente: 0 para c√°mara web, o ruta a archivo de video
        """
        if self.modelo_activo is None:
            print("No hay modelo activo. Seleccione un modelo primero.")
            return
        
        print(f"Iniciando detecci√≥n con modelo: {self.modelo_activo}")
        print("Presione 'q' para salir, 'c' para cambiar configuraci√≥n")
        
        cap = cv2.VideoCapture(fuente)
        
        if not cap.isOpened():
            print("Error: No se pudo abrir la fuente de video")
            return
        
        # Configurar FPS
        fps = int(cap.get(cv2.CAP_PROP_FPS)) or self.configuracion['fps_objetivo']
        delay = 1.0 / fps
        
        # Variables de rendimiento
        tiempo_anterior = time.time()
        contador_frames = 0
        fps_real = 0
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                inicio_deteccion = time.time()
                
                # Realizar detecci√≥n
                detecciones = self.detectar_en_frame(frame, self.modelo_activo)
                
                # Dibujar resultados
                frame_result = self.dibujar_detecciones(frame, detecciones)
                
                # Calcular FPS
                tiempo_actual = time.time()
                contador_frames += 1
                if tiempo_actual - tiempo_anterior >= 1.0:
                    fps_real = contador_frames
                    contador_frames = 0
                    tiempo_anterior = tiempo_actual
                
                # Informaci√≥n de rendimiento
                tiempo_deteccion = time.time() - inicio_deteccion
                info_texto = f"Modelo: {self.modelo_activo} | FPS: {fps_real} | Tiempo: {tiempo_deteccion*1000:.1f}ms"
                cv2.putText(frame_result, info_texto, (10, frame_result.shape[0] - 20),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                
                # Mostrar frame
                cv2.imshow('Detecci√≥n de Sombreros - Presione Q para salir', frame_result)
                
                # Control de teclado
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    break
                elif key == ord('c'):
                    self._cambiar_configuracion_tiempo_real()
                
                # Control de FPS
                tiempo_restante = delay - (time.time() - inicio_deteccion)
                if tiempo_restante > 0:
                    time.sleep(tiempo_restante)
        
        except KeyboardInterrupt:
            print("Detecci√≥n interrumpida por el usuario")
        
        finally:
            cap.release()
            cv2.destroyAllWindows()
            print("Procesamiento de video finalizado")
    
    def _cambiar_configuracion_tiempo_real(self):
        """Permite cambiar configuraci√≥n durante la ejecuci√≥n."""
        print("\nConfiguraci√≥n actual:")
        print(f"Umbral de confianza: {self.configuracion['umbral_confianza']}")
        print(f"Mostrar confianza: {self.configuracion['mostrar_confianza']}")
        
        # Cambiar umbral de confianza
        try:
            nuevo_umbral = float(input("Nuevo umbral de confianza (0.1-0.9): "))
            if 0.1 <= nuevo_umbral <= 0.9:
                self.configuracion['umbral_confianza'] = nuevo_umbral
                print("Umbral actualizado")
        except ValueError:
            print("Valor inv√°lido")
    
    def mostrar_menu_modelos(self):
        """Muestra el men√∫ de selecci√≥n de modelos."""
        print("\n--- SELECCI√ìN DE MODELO ---")
        print("Clasificaci√≥n:")
        for key, desc in self.modelos_disponibles['clasificacion'].items():
            estado = "(CARGADO)" if key in self.modelos_cargados else ""
            print(f"  {key}: {desc} {estado}")
        
        print("\nDetecci√≥n:")
        for key, desc in self.modelos_disponibles['deteccion'].items():
            estado = "(CARGADO)" if key in self.modelos_cargados else ""
            print(f"  {key}: {desc} {estado}")
        
        print("\nSegmentaci√≥n:")
        for key, desc in self.modelos_disponibles['segmentacion'].items():
            estado = "(CARGADO)" if key in self.modelos_cargados else ""
            print(f"  {key}: {desc} {estado}")
        
        if self.modelo_activo:
            print(f"\nModelo activo: {self.modelo_activo}")
    
    def ejecutar_menu_principal(self):
        """Ejecuta el men√∫ principal del sistema."""
        while True:
            print("\n" + "="*50)
            print("SISTEMA DE DETECCI√ìN DE SOMBREROS EN VIDEO")
            print("="*50)
            print("1. Cargar modelo")
            print("2. Detecci√≥n en tiempo real (c√°mara)")
            print("3. Procesar archivo de video")
            print("4. Configurar par√°metros")
            print("5. Ver modelos disponibles")
            print("6. Ver informaci√≥n del sistema")
            print("7. üîß Solucionar problemas DeepLabV3")
            print("0. Salir")
            print("="*50)
            
            try:
                opcion = input("Seleccione una opci√≥n: ").strip()
                
                if opcion == '1':
                    self._menu_cargar_modelo()
                elif opcion == '2':
                    self.procesar_video_tiempo_real(0)
                elif opcion == '3':
                    self._procesar_archivo_video()
                elif opcion == '4':
                    self._menu_configuracion()
                elif opcion == '5':
                    self.mostrar_menu_modelos()
                elif opcion == '6':
                    self._mostrar_info_sistema()
                elif opcion == '7':
                    self.explicar_problema_deeplabv3()
                elif opcion == '0':
                    print("Saliendo del sistema...")
                    break
                else:
                    print("Opci√≥n inv√°lida")
                    
            except KeyboardInterrupt:
                print("\nSaliendo del sistema...")
                break
            except Exception as e:
                print(f"Error: {e}")
    
    def _menu_cargar_modelo(self):
        """Men√∫ para cargar modelos."""
        self.mostrar_menu_modelos()
        
        modelo = input("\nIngrese el nombre del modelo a cargar: ").strip().lower()
        
        # Verificar si el modelo existe
        todos_modelos = {}
        for categoria in self.modelos_disponibles.values():
            todos_modelos.update(categoria)
        
        if modelo in todos_modelos:
            if self.cargar_modelo(modelo):
                print(f"Modelo {modelo} cargado exitosamente")
            else:
                print(f"Error cargando modelo {modelo}")
        else:
            print("Modelo no encontrado")
    
    def _procesar_archivo_video(self):
        """Procesa un archivo de video con selecci√≥n interactiva."""
        print("\n" + "="*60)
        print("SELECCI√ìN DE VIDEO")
        print("="*60)
        
        # Solicitar carpeta al usuario
        print("\nüìÅ Ingrese la ruta de la carpeta con videos")
        print("   (Presione Enter para usar carpetas por defecto)")
        print("   Carpetas por defecto: videos, data/videos, images, .")
        carpeta_usuario = input("\nRuta de carpeta: ").strip()
        
        # Determinar carpetas a buscar
        if carpeta_usuario:
            # Usuario especific√≥ una carpeta
            if os.path.exists(carpeta_usuario):
                if os.path.isfile(carpeta_usuario):
                    # Si es un archivo directamente, procesarlo
                    print(f"\n‚úì Archivo especificado directamente: {os.path.basename(carpeta_usuario)}")
                    ruta_video = carpeta_usuario
                    self._mostrar_info_y_procesar_video(ruta_video)
                    return
                else:
                    # Es una carpeta
                    carpetas_video = [carpeta_usuario]
            else:
                print(f"‚ùå La carpeta '{carpeta_usuario}' no existe")
                return
        else:
            # Usar carpetas por defecto
            carpetas_video = ['videos', 'data/videos', 'images', '.']
        
        # Buscar videos en las carpetas
        videos_encontrados = []
        print(f"\nüîç Buscando videos en: {', '.join(carpetas_video)}")
        
        for carpeta in carpetas_video:
            if os.path.exists(carpeta):
                # Buscar archivos de video
                extensiones = ['*.avi', '*.mp4', '*.mov', '*.mkv', '*.flv', '*.wmv']
                for ext in extensiones:
                    ruta_busqueda = os.path.join(carpeta, ext)
                    videos = glob.glob(ruta_busqueda)
                    videos_encontrados.extend(videos)
        
        if not videos_encontrados:
            print("\n‚ö†Ô∏è  No se encontraron videos en las carpetas especificadas")
            print("\n¬øDesea ingresar la ruta completa del archivo? (s/n): ", end="")
            if input().lower() in ['s', 'si', 's√≠', 'y', 'yes']:
                ruta_video = input("Ingrese la ruta completa del archivo de video: ").strip()
                if os.path.exists(ruta_video):
                    self._mostrar_info_y_procesar_video(ruta_video)
                else:
                    print(f"‚ùå Archivo no encontrado: {ruta_video}")
            return
        
        # Mostrar lista de videos encontrados
        print(f"\nüìπ Videos encontrados: {len(videos_encontrados)}")
        print("-" * 60)
        
        for i, video in enumerate(videos_encontrados, 1):
            try:
                tama√±o = os.path.getsize(video) / (1024 * 1024)  # MB
                print(f"{i}. {os.path.basename(video)}")
                print(f"   üìÇ Ruta: {video}")
                print(f"   üíæ Tama√±o: {tama√±o:.2f} MB")
                print()
            except Exception as e:
                print(f"{i}. {os.path.basename(video)}")
                print(f"   üìÇ Ruta: {video}")
                print(f"   ‚ö†Ô∏è  Error obteniendo informaci√≥n: {e}")
                print()
        
        print("0. Ingresar ruta manualmente")
        print("-" * 60)
        
        try:
            seleccion = int(input("\nüéØ Seleccione el n√∫mero del video: ").strip())
            
            if seleccion == 0:
                ruta_video = input("Ingrese la ruta completa del archivo de video: ").strip()
            elif 1 <= seleccion <= len(videos_encontrados):
                ruta_video = videos_encontrados[seleccion - 1]
                print(f"\n‚úì Video seleccionado: {os.path.basename(ruta_video)}")
            else:
                print("‚ùå Selecci√≥n inv√°lida")
                return
                
        except ValueError:
            print("‚ùå Entrada inv√°lida")
            return
        
        # Verificar que el archivo existe
        if not os.path.exists(ruta_video):
            print(f"‚ùå Archivo no encontrado: {ruta_video}")
            return
        
        # Mostrar informaci√≥n y procesar
        self._mostrar_info_y_procesar_video(ruta_video)
    
    def _mostrar_info_y_procesar_video(self, ruta_video):
        """Muestra informaci√≥n del video y lo procesa."""
        # Mostrar informaci√≥n del video
        try:
            cap_test = cv2.VideoCapture(ruta_video)
            if cap_test.isOpened():
                fps = int(cap_test.get(cv2.CAP_PROP_FPS))
                frames = int(cap_test.get(cv2.CAP_PROP_FRAME_COUNT))
                ancho = int(cap_test.get(cv2.CAP_PROP_FRAME_WIDTH))
                alto = int(cap_test.get(cv2.CAP_PROP_FRAME_HEIGHT))
                duracion = frames / fps if fps > 0 else 0
                
                print(f"\nüìä Informaci√≥n del video:")
                print(f"   üñºÔ∏è  Resoluci√≥n: {ancho}x{alto}")
                print(f"   üé¨ FPS: {fps}")
                print(f"   üìΩÔ∏è  Frames totales: {frames}")
                print(f"   ‚è±Ô∏è  Duraci√≥n: {duracion:.2f} segundos")
                
                cap_test.release()
            else:
                print("\n‚ö†Ô∏è  No se pudo leer informaci√≥n del video")
        except Exception as e:
            print(f"\n‚ö†Ô∏è  Error leyendo informaci√≥n: {e}")
        
        # Confirmar procesamiento
        confirmar = input("\n¬øProcesar este video? (s/n): ").strip().lower()
        if confirmar in ['s', 'si', 's√≠', 'y', 'yes']:
            print("\nüé¨ Iniciando procesamiento de video...")
            print("Presione 'q' para detener\n")
            self.procesar_video_tiempo_real(ruta_video)
        else:
            print("Procesamiento cancelado")
    
    def _menu_configuracion(self):
        """Men√∫ de configuraci√≥n."""
        print("\n--- CONFIGURACI√ìN ---")
        print(f"1. Umbral de confianza: {self.configuracion['umbral_confianza']}")
        print(f"2. FPS objetivo: {self.configuracion['fps_objetivo']}")
        print(f"3. Mostrar confianza: {self.configuracion['mostrar_confianza']}")
        print("0. Volver")
        
        opcion = input("Seleccione opci√≥n: ").strip()
        
        try:
            if opcion == '1':
                nuevo_umbral = float(input("Nuevo umbral (0.1-0.9): "))
                if 0.1 <= nuevo_umbral <= 0.9:
                    self.configuracion['umbral_confianza'] = nuevo_umbral
                    print("Umbral actualizado")
                    
            elif opcion == '2':
                nuevo_fps = int(input("Nuevo FPS objetivo (10-60): "))
                if 10 <= nuevo_fps <= 60:
                    self.configuracion['fps_objetivo'] = nuevo_fps
                    print("FPS actualizado")
                    
            elif opcion == '3':
                self.configuracion['mostrar_confianza'] = not self.configuracion['mostrar_confianza']
                print(f"Mostrar confianza: {self.configuracion['mostrar_confianza']}")
                
        except ValueError:
            print("Valor inv√°lido")
    
    def _mostrar_info_sistema(self):
        """Muestra informaci√≥n del sistema."""
        print("\n--- INFORMACI√ìN DEL SISTEMA ---")
        print(f"TensorFlow: {tf.__version__}")
        if TORCH_AVAILABLE:
            print(f"PyTorch: {torch.__version__}")
        else:
            print("PyTorch: No disponible")
        
        print(f"OpenCV: {cv2.__version__}")
        print(f"Modelos cargados: {len(self.modelos_cargados)}")
        print(f"Modelo activo: {self.modelo_activo or 'Ninguno'}")
        
        # Informaci√≥n de GPU
        gpus = tf.config.list_physical_devices('GPU')
        print(f"GPUs disponibles: {len(gpus)}")

def main():
    """Funci√≥n principal."""
    print("Inicializando Sistema de Detecci√≥n de Video...")
    
    detector = DetectorVideoModelos()
    detector.ejecutar_menu_principal()

if __name__ == "__main__":
    main()