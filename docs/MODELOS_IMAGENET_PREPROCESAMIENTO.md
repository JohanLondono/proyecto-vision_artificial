# Modelos ImageNet: Preprocesamiento y Diferencias

## Universidad del QuindÃ­o - VisiÃ³n Artificial
**Fecha:** Noviembre 2025

---

## ğŸ“‹ Tabla de Contenidos

1. [IntroducciÃ³n a ImageNet](#introducciÃ³n-a-imagenet)
2. [Modelos Implementados](#modelos-implementados)
3. [Diferencias CrÃ­ticas de Preprocesamiento](#diferencias-crÃ­ticas-de-preprocesamiento)
4. [ComparaciÃ³n Detallada](#comparaciÃ³n-detallada)
5. [CÃ³digo de ImplementaciÃ³n](#cÃ³digo-de-implementaciÃ³n)
6. [Errores Comunes](#errores-comunes)

---

## ğŸ¯ IntroducciÃ³n a ImageNet

**ImageNet** es un proyecto de base de datos de imÃ¡genes a gran escala diseÃ±ado para investigaciÃ³n en reconocimiento visual de objetos. Contiene:

- **1000 categorÃ­as/clases** de objetos
- MÃ¡s de **14 millones de imÃ¡genes** etiquetadas
- Base de datos estÃ¡ndar para entrenar y evaluar modelos de visiÃ³n por computadora

### Â¿Por quÃ© ImageNet es importante?

Los modelos preentrenados en ImageNet han aprendido caracterÃ­sticas visuales generales que pueden transferirse a otras tareas mediante **Transfer Learning**. Estos modelos pueden:

1. **Clasificar** imÃ¡genes en 1000 categorÃ­as
2. **Extraer caracterÃ­sticas** Ãºtiles para otras tareas
3. **Servir como base** para fine-tuning en dominios especÃ­ficos

---

## ğŸ§  Modelos Implementados

### 1. VGG16 (Visual Geometry Group - 16 capas)

**CaracterÃ­sticas:**
- **AÃ±o:** 2014
- **Profundidad:** 16 capas con pesos entrenables
- **ParÃ¡metros:** ~138 millones
- **Arquitectura:** Bloques repetitivos de convoluciones 3x3

**Ventajas:**
- âœ… Arquitectura simple y uniforme
- âœ… Excelente para extracciÃ³n de caracterÃ­sticas
- âœ… FÃ¡cil de entender e implementar

**Desventajas:**
- âŒ Muy pesado (muchos parÃ¡metros)
- âŒ Lento en inferencia
- âŒ Consume mucha memoria

**Funcionamiento:**
```
Entrada (224x224x3)
    â†“
[Conv 3x3] Ã— 2 â†’ MaxPool â†’ [Conv 3x3] Ã— 2 â†’ MaxPool
    â†“
[Conv 3x3] Ã— 3 â†’ MaxPool â†’ [Conv 3x3] Ã— 3 â†’ MaxPool
    â†“
[Conv 3x3] Ã— 3 â†’ MaxPool â†’ FC Layers â†’ Softmax (1000 clases)
```

---

### 2. ResNet50 (Residual Network - 50 capas)

**CaracterÃ­sticas:**
- **AÃ±o:** 2015
- **Profundidad:** 50 capas
- **ParÃ¡metros:** ~25 millones
- **InnovaciÃ³n:** Conexiones residuales (skip connections)

**Ventajas:**
- âœ… Resuelve el problema del desvanecimiento del gradiente
- âœ… MÃ¡s profundo pero mÃ¡s eficiente que VGG
- âœ… Excelente balance precisiÃ³n/velocidad
- âœ… Menos parÃ¡metros que VGG16

**Desventajas:**
- âŒ MÃ¡s complejo de entender
- âŒ Arquitectura mÃ¡s sofisticada

**Funcionamiento con Bloques Residuales:**
```
Bloque Residual:
    x â†’ [Conv] â†’ [BN] â†’ [ReLU] â†’ [Conv] â†’ [BN] â†’ (+) â†’ [ReLU]
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             (ConexiÃ³n de atajo/skip)
```

**Â¿Por quÃ© funciona?**
Las conexiones residuales permiten que el gradiente fluya directamente hacia atrÃ¡s, facilitando el entrenamiento de redes muy profundas.

---

### 3. ResNet101 V2 (Residual Network - 101 capas, VersiÃ³n 2)

**CaracterÃ­sticas:**
- **AÃ±o:** 2016 (mejora de ResNet original)
- **Profundidad:** 101 capas
- **ParÃ¡metros:** ~44 millones
- **InnovaciÃ³n:** PreactivaciÃ³n (BatchNorm â†’ ReLU â†’ Conv)

**Ventajas:**
- âœ… Mejor rendimiento que ResNet V1
- âœ… Entrenamiento mÃ¡s estable
- âœ… Mayor capacidad de representaciÃ³n
- âœ… Mejor propagaciÃ³n del gradiente

**Desventajas:**
- âŒ MÃ¡s lento que ResNet50
- âŒ MÃ¡s parÃ¡metros
- âŒ Requiere mÃ¡s memoria

**Diferencia clave con ResNet V1:**
```
ResNet V1:           x â†’ [Conv] â†’ [BN] â†’ [ReLU] â†’ ...
ResNet V2 (mejor):   x â†’ [BN] â†’ [ReLU] â†’ [Conv] â†’ ...
```

La preactivaciÃ³n (BN y ReLU antes de Conv) mejora el flujo de gradientes.

---

## âš™ï¸ Diferencias CrÃ­ticas de Preprocesamiento

### ğŸ”´ **ESTO ES CRÃTICO:** Orden de Canales de Color

Los modelos de ImageNet se entrenaron con diferentes bibliotecas que usan Ã³rdenes de canales distintos:

| Modelo | Framework Original | Orden de Canales | Modo |
|--------|-------------------|------------------|------|
| **VGG16** | Caffe | **BGR** | Caffe |
| **ResNet50** | Caffe | **BGR** | Caffe |
| **ResNet101 V2** | TensorFlow | **RGB** | Torch |

### ğŸ“Š Modos de Preprocesamiento

#### Modo Caffe (VGG16, ResNet50)

**Entrada esperada:** BGR [0, 255]

**TransformaciÃ³n:**
```python
# SubstracciÃ³n de medias de ImageNet (en BGR)
mean = [103.939, 116.779, 123.68]  # BGR
preprocessed = image - mean

# Resultado: valores aproximados en [-128, 128]
```

**Valores de media (calculados del dataset ImageNet):**
- Canal B (Blue): 103.939
- Canal G (Green): 116.779  
- Canal R (Red): 123.68

#### Modo Torch (ResNet V2)

**Entrada esperada:** RGB [0, 255]

**TransformaciÃ³n:**
```python
# NormalizaciÃ³n a [-1, 1]
preprocessed = (image / 127.5) - 1

# Equivalente a:
# preprocessed = (image / 255.0) * 2 - 1

# Resultado: valores en [-1, 1]
```

---

## ğŸ“Š ComparaciÃ³n Detallada

### Tabla Comparativa Completa

| CaracterÃ­stica | VGG16 | ResNet50 | ResNet101 V2 |
|---------------|-------|----------|--------------|
| **AÃ±o** | 2014 | 2015 | 2016 |
| **Capas** | 16 | 50 | 101 |
| **ParÃ¡metros** | ~138M | ~25M | ~44M |
| **Top-1 Accuracy** | 71.3% | 76.0% | 77.8% |
| **Top-5 Accuracy** | 90.1% | 93.0% | 93.8% |
| **TamaÃ±o Modelo** | 528 MB | 98 MB | 171 MB |
| **Velocidad Inferencia** | Lenta | Media | Media-Lenta |
| **Preprocesamiento** | Caffe (BGR) | Caffe (BGR) | Torch (RGB) |
| **Rango Valores** | ~[-128, 128] | ~[-128, 128] | [-1, 1] |

### Rendimiento por CategorÃ­as

**VGG16:**
- ğŸ¯ Bueno en: Objetos grandes, escenas simples
- âš ï¸ DÃ©bil en: Objetos pequeÃ±os, escenas complejas

**ResNet50:**
- ğŸ¯ Bueno en: Balance general, objetos variados
- âš ï¸ DÃ©bil en: Detalles muy finos

**ResNet101 V2:**
- ğŸ¯ Bueno en: Objetos complejos, detalles finos
- âš ï¸ DÃ©bil en: Velocidad de inferencia

---

## ğŸ’» CÃ³digo de ImplementaciÃ³n

### Preprocesamiento Correcto

```python
import cv2
import numpy as np
from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess
from tensorflow.keras.applications.resnet50 import preprocess_input as resnet50_preprocess
from tensorflow.keras.applications.resnet_v2 import preprocess_input as resnetv2_preprocess

def preprocesar_imagen_modelo(imagen_rgb, modelo_nombre):
    """
    Preprocesa una imagen segÃºn el modelo especÃ­fico.
    
    Args:
        imagen_rgb: Imagen en formato RGB [0, 255]
        modelo_nombre: 'vgg16', 'resnet50', o 'resnet101'
    
    Returns:
        Imagen preprocesada lista para el modelo
    """
    # Redimensionar a 224x224
    imagen_resized = cv2.resize(imagen_rgb, (224, 224))
    
    if modelo_nombre in ['vgg16', 'resnet50']:
        # Modo Caffe: Necesita BGR
        imagen_bgr = cv2.cvtColor(imagen_resized, cv2.COLOR_RGB2BGR)
        imagen_batch = np.expand_dims(imagen_bgr, axis=0)
        
        if modelo_nombre == 'vgg16':
            return vgg_preprocess(imagen_batch.copy())
        else:  # resnet50
            return resnet50_preprocess(imagen_batch.copy())
    
    else:  # resnet101 (V2)
        # Modo Torch: Usa RGB directamente
        imagen_batch = np.expand_dims(imagen_resized, axis=0)
        return resnetv2_preprocess(imagen_batch.copy())
```

### Carga de Modelos

```python
from tensorflow.keras.applications import VGG16, ResNet50, ResNet101V2
from tensorflow.keras.applications.imagenet_utils import decode_predictions

# Cargar modelos con pesos de ImageNet
vgg16 = VGG16(weights='imagenet', include_top=True, input_shape=(224, 224, 3))
resnet50 = ResNet50(weights='imagenet', include_top=True, input_shape=(224, 224, 3))
resnet101 = ResNet101V2(weights='imagenet', include_top=True, input_shape=(224, 224, 3))

print("âœ“ Modelos cargados con 1000 clases de ImageNet")
```

### PredicciÃ³n y DecodificaciÃ³n

```python
def predecir_imagenet(imagen_rgb, modelo, modelo_nombre):
    """
    Realiza predicciÃ³n con un modelo de ImageNet.
    
    Args:
        imagen_rgb: Imagen en RGB [0, 255]
        modelo: Modelo cargado
        modelo_nombre: Nombre del modelo
    
    Returns:
        Diccionario con predicciones decodificadas
    """
    # Preprocesar
    imagen_prep = preprocesar_imagen_modelo(imagen_rgb, modelo_nombre)
    
    # Predecir
    predicciones = modelo.predict(imagen_prep)
    
    # Decodificar (convierte Ã­ndices a nombres de clases)
    decoded = decode_predictions(predicciones, top=5)[0]
    
    # Formatear resultados
    resultados = []
    for id_clase, nombre_clase, confianza in decoded:
        resultados.append({
            'id': id_clase,
            'clase': nombre_clase.replace('_', ' ').title(),
            'confianza': float(confianza)
        })
    
    return resultados
```

---

## âš ï¸ Errores Comunes

### Error #1: Usar mismo preprocesamiento para todos los modelos

**âŒ INCORRECTO:**
```python
# Esto causarÃ¡ resultados errÃ³neos en ResNet V2
imagen_prep = preprocess_input(imagen)  # Â¿QuÃ© funciÃ³n es esta?
pred_vgg = vgg16.predict(imagen_prep)
pred_resnet101 = resnet101.predict(imagen_prep)  # âŒ INCORRECTO
```

**âœ… CORRECTO:**
```python
# Preprocesamiento especÃ­fico para cada modelo
imagen_prep_vgg = vgg_preprocess(imagen_bgr)
imagen_prep_resnet101 = resnetv2_preprocess(imagen_rgb)

pred_vgg = vgg16.predict(imagen_prep_vgg)
pred_resnet101 = resnet101.predict(imagen_prep_resnet101)
```

### Error #2: Usar RGB para VGG16/ResNet50

**âŒ INCORRECTO:**
```python
imagen = cv2.imread('foto.jpg')
imagen_rgb = cv2.cvtColor(imagen, cv2.COLOR_BGR2RGB)
# Usar imagen_rgb directamente para VGG16 âŒ
pred = vgg16.predict(vgg_preprocess(np.expand_dims(imagen_rgb, 0)))
```

**âœ… CORRECTO:**
```python
imagen = cv2.imread('foto.jpg')
imagen_rgb = cv2.cvtColor(imagen, cv2.COLOR_BGR2RGB)
# Convertir de vuelta a BGR para VGG16
imagen_bgr = cv2.cvtColor(imagen_rgb, cv2.COLOR_RGB2BGR)
pred = vgg16.predict(vgg_preprocess(np.expand_dims(imagen_bgr, 0)))
```

### Error #3: Usar BGR para ResNet V2

**âŒ INCORRECTO:**
```python
imagen = cv2.imread('foto.jpg')  # BGR por defecto
# Usar BGR directamente para ResNet V2 âŒ
pred = resnet101.predict(resnetv2_preprocess(np.expand_dims(imagen, 0)))
```

**âœ… CORRECTO:**
```python
imagen = cv2.imread('foto.jpg')
imagen_rgb = cv2.cvtColor(imagen, cv2.COLOR_BGR2RGB)
pred = resnet101.predict(resnetv2_preprocess(np.expand_dims(imagen_rgb, 0)))
```

### Error #4: No usar decode_predictions

**âŒ INCORRECTO:**
```python
predicciones = modelo.predict(imagen_prep)
print(predicciones)  # Imprime array de 1000 valores
# Salida: [0.001, 0.002, 0.997, ...] âŒ No interpretable
```

**âœ… CORRECTO:**
```python
predicciones = modelo.predict(imagen_prep)
decoded = decode_predictions(predicciones, top=5)[0]
for id_clase, nombre, conf in decoded:
    print(f"{nombre}: {conf:.3f}")
# Salida: "Golden Retriever: 0.997" âœ… Interpretable
```

---

## ğŸ” VerificaciÃ³n de Preprocesamiento

### Script de Prueba

```python
import numpy as np
from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_prep
from tensorflow.keras.applications.resnet50 import preprocess_input as resnet50_prep
from tensorflow.keras.applications.resnet_v2 import preprocess_input as resnetv2_prep

# Imagen de prueba [0-255]
test_img = np.array([[[[100, 150, 200]]]], dtype=np.float32)

print("Imagen original (RGB):", test_img[0,0,0])

# VGG16
vgg_out = vgg_prep(test_img.copy())
print(f"VGG16: {vgg_out[0,0,0]}")
print(f"  Rango: [{vgg_out.min():.1f}, {vgg_out.max():.1f}]")

# ResNet50
resnet50_out = resnet50_prep(test_img.copy())
print(f"ResNet50: {resnet50_out[0,0,0]}")
print(f"  Rango: [{resnet50_out.min():.1f}, {resnet50_out.max():.1f}]")

# ResNet V2
resnetv2_out = resnetv2_prep(test_img.copy())
print(f"ResNet V2: {resnetv2_out[0,0,0]}")
print(f"  Rango: [{resnetv2_out.min():.1f}, {resnetv2_out.max():.1f}]")
```

**Salida esperada:**
```
Imagen original (RGB): [100. 150. 200.]
VGG16: [ 96.061  33.221 -23.68 ]
  Rango: [-23.7, 96.1]
ResNet50: [ 96.061  33.221 -23.68 ]
  Rango: [-23.7, 96.1]
ResNet V2: [-0.216  0.176  0.569]
  Rango: [-1.0, 1.0]
```

---

## ğŸ“š Referencias

1. **VGG16:** Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition.
2. **ResNet:** He, K., et al. (2015). Deep residual learning for image recognition.
3. **ResNet V2:** He, K., et al. (2016). Identity mappings in deep residual networks.
4. **ImageNet:** Deng, J., et al. (2009). ImageNet: A large-scale hierarchical image database.

---

## ğŸ“ Resumen Ejecutivo

### Lo que DEBES recordar:

1. **VGG16 y ResNet50** â†’ Modo Caffe â†’ **BGR** â†’ SubstracciÃ³n de medias
2. **ResNet101 V2** â†’ Modo Torch â†’ **RGB** â†’ NormalizaciÃ³n [-1, 1]
3. Siempre usar el **preprocesamiento correcto** para cada modelo
4. **decode_predictions** convierte Ã­ndices de clase a nombres legibles
5. **include_top=True** para usar las 1000 clases completas de ImageNet

### Flujo correcto:

```
OpenCV imread â†’ BGR
    â†“
cv2.cvtColor â†’ RGB
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   VGG16       â”‚   ResNet50       â”‚   ResNet101 V2  â”‚
â”‚   RGB â†’ BGR   â”‚   RGB â†’ BGR      â”‚   RGB (directo) â”‚
â”‚   vgg_prep    â”‚   resnet50_prep  â”‚   resnetv2_prep â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“               â†“                   â†“
  PredicciÃ³n    PredicciÃ³n          PredicciÃ³n
    â†“               â†“                   â†“
      decode_predictions (Top 5 clases)
```

---

## ğŸ¨ Modelos de SegmentaciÃ³n

AdemÃ¡s de los modelos de clasificaciÃ³n de ImageNet, el sistema incluye modelos de segmentaciÃ³n semÃ¡ntica y de instancias para detectar y delimitar objetos pixel por pixel.

---

### U-Net con Encoder Preentrenado

**Tipo:** SegmentaciÃ³n SemÃ¡ntica  
**AÃ±o:** 2015 (arquitectura original), mejorado con encoders modernos  
**Framework:** TensorFlow/Keras

**DescripciÃ³n:**
U-Net es una arquitectura de red neuronal convolucional diseÃ±ada especÃ­ficamente para segmentaciÃ³n de imÃ¡genes. Su nombre proviene de su forma de "U" cuando se visualiza la arquitectura.

**ImplementaciÃ³n en este proyecto:**

1. **OpciÃ³n Principal:** DeepLabV3 desde TensorFlow Hub
   - Modelo completo preentrenado en PASCAL VOC
   - Arquitectura de Ãºltima generaciÃ³n con ASPP (Atrous Spatial Pyramid Pooling)
   - SegmentaciÃ³n semÃ¡ntica de alta calidad

2. **OpciÃ³n Alternativa:** U-Net con ResNet50 preentrenado
   - Encoder: ResNet50 con pesos de ImageNet (congelado)
   - Decoder: Capas de upsampling con skip connections
   - Combina caracterÃ­sticas de bajo y alto nivel

**Arquitectura U-Net:**
```
Entrada (224x224x3)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ENCODER (ResNet50 preentrenado)    â”‚
â”‚                                     â”‚
â”‚ Conv2D + BN + ReLU â†’ 112x112 â”€â”€â”   â”‚
â”‚         â†“                       â”‚   â”‚
â”‚ MaxPool â†’ 56x56 â”€â”€â”            â”‚   â”‚
â”‚         â†“         â”‚            â”‚   â”‚
â”‚ Conv Blocks â†’ 28x28 â”€â”         â”‚   â”‚
â”‚         â†“           â”‚         â”‚   â”‚
â”‚ Conv Blocks â†’ 14x14 â”€â”         â”‚   â”‚
â”‚         â†“           â”‚         â”‚   â”‚
â”‚ Bottleneck â†’ 7x7    â”‚         â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   â”‚
           â†“                    â”‚   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”¼â”
â”‚ DECODER                      â”‚   â”‚â”‚
â”‚                              â”‚   â”‚â”‚
â”‚ UpSample + Concat â† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚â”‚
â”‚         â†“                        â”‚â”‚
â”‚ Conv2D Ã— 2 (14x14)               â”‚â”‚
â”‚         â†“                        â”‚â”‚
â”‚ UpSample + Concat â† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”˜â”‚
â”‚         â†“                          â”‚
â”‚ Conv2D Ã— 2 (28x28)                 â”‚
â”‚         â†“                          â”‚
â”‚ UpSample + Concat â† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚         â†“
â”‚ Conv2D Ã— 2 (56x56)
â”‚         â†“
â”‚ UpSample (112x112)
â”‚         â†“
â”‚ Conv2D Ã— 2 (224x224)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
    MÃ¡scara (224x224x1)
```

**CaracterÃ­sticas Clave:**

1. **Skip Connections:** Conecta capas del encoder con el decoder
   - Preserva detalles espaciales finos
   - Combina caracterÃ­sticas de diferentes niveles
   - Mejora la precisiÃ³n de los bordes

2. **Encoder Preentrenado:** Usa ResNet50 con pesos de ImageNet
   - Extrae caracterÃ­sticas visuales robustas
   - Reduce el tiempo de entrenamiento
   - Mejora el rendimiento con pocos datos

3. **Decoder SimÃ©trico:** Reconstruye la resoluciÃ³n espacial
   - Upsampling bilineal para suavidad
   - Convoluciones para refinar detalles
   - Salida con resoluciÃ³n completa

**Ventajas:**
- âœ… Excelente para segmentaciÃ³n de objetos
- âœ… Encoder preentrenado (no aleatorio)
- âœ… Funciona bien con pocos datos de entrenamiento
- âœ… Skip connections preservan detalles
- âœ… Salida de alta resoluciÃ³n

**Desventajas:**
- âŒ MÃ¡s lento que modelos de clasificaciÃ³n
- âŒ Requiere mÃ¡s memoria
- âŒ SegmentaciÃ³n binaria (objeto vs. fondo)

**MÃ©tricas de EvaluaciÃ³n:**

```python
# MÃ©tricas proporcionadas por U-Net
{
    'tipo': 'segmentacion_unet',
    'clase': 'objeto_grande',  # grande/mediano/pequeÃ±o/sin_objeto
    'confianza': 0.89,
    'bbox': [x, y, width, height],
    'segmentacion': mascara_binaria,  # Array numpy
    'area_segmentada': 15420,  # pÃ­xeles
    'porcentaje': 30.1,  # % de la imagen
    'num_objetos': 1,
    'metricas': {
        'area_contorno_principal': 15300,
        'area_bbox': 18000,
        'densidad': 85.0,  # % de relleno del bbox
        'pixeles_totales': 50176
    }
}
```

**Preprocesamiento:**
```python
# U-Net usa el preprocesamiento de ResNet50 (modo Caffe)
imagen_bgr = cv2.cvtColor(imagen_rgb, cv2.COLOR_RGB2BGR)
imagen_batch = np.expand_dims(cv2.resize(imagen_bgr, (224, 224)), axis=0)
imagen_prep = preprocess_input(imagen_batch)  # Substrae medias ImageNet
```

---

### Mask R-CNN / DeepLabV3+

**Tipo:** SegmentaciÃ³n de Instancias / SegmentaciÃ³n SemÃ¡ntica  
**AÃ±o:** 2017 (Mask R-CNN), 2018 (DeepLabV3+)  
**Framework:** Detectron2 / TensorFlow

**DescripciÃ³n:**
El sistema intenta usar Mask R-CNN real (detectron2) para segmentaciÃ³n de instancias, pero si no estÃ¡ disponible, usa DeepLabV3+ como alternativa con resultados comparables.

**ImplementaciÃ³n:**

#### OpciÃ³n 1: Mask R-CNN (Detectron2)

Si detectron2 estÃ¡ instalado:
- Modelo completo preentrenado en COCO (80 clases)
- Detecta y segmenta mÃºltiples instancias simultÃ¡neamente
- Proporciona bbox, clase y mÃ¡scara para cada instancia
- Basado en Faster R-CNN + branch de segmentaciÃ³n

**Arquitectura Mask R-CNN:**
```
Entrada
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Backbone (ResNet50-FPN) â”‚
â”‚ ExtracciÃ³n de features  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RPN (Region Proposal Net)â”‚
â”‚ Genera propuestas de bboxâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚
    â†“             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Box Headâ”‚  â”‚Mask Headâ”‚
â”‚ Clasif. â”‚  â”‚Segment. â”‚
â”‚ + Bbox  â”‚  â”‚  MÃ¡scaraâ”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚            â”‚
     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â†“
   Detecciones con MÃ¡scaras
```

**Salida Mask R-CNN:**
```python
{
    'tipo': 'segmentacion_instancias',
    'clase': 'person',  # De 80 clases COCO
    'confianza': 0.95,
    'bbox': [x, y, w, h],
    'segmentacion': mascara_binaria,
    'num_instancias': 3,
    'instancias': [
        {
            'bbox': [100, 50, 80, 150],
            'clase_id': 0,
            'confianza': 0.95,
            'mascara': array(...)
        },
        # ... mÃ¡s instancias
    ]
}
```

#### OpciÃ³n 2: DeepLabV3+ (TensorFlow)

Si detectron2 no estÃ¡ disponible (Windows):
- SegmentaciÃ³n semÃ¡ntica con arquitectura ASPP
- Preentrenado con ResNet50 en ImageNet
- 21 clases de PASCAL VOC incluyendo personas
- Alta calidad de bordes

**Arquitectura DeepLabV3+:**
```
Entrada (224x224x3)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Encoder (ResNet50)        â”‚
â”‚ ExtracciÃ³n de features    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“ (7x7)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ASPP (Atrous Spatial Pyramid)    â”‚
â”‚                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”   â”‚
â”‚  â”‚1x1 â”‚ â”‚3x3 â”‚ â”‚3x3 â”‚ â”‚3x3 â”‚   â”‚
â”‚  â”‚convâ”‚ â”‚r=6 â”‚ â”‚r=12â”‚ â”‚r=18â”‚   â”‚
â”‚  â””â”€â”¬â”€â”€â”˜ â””â”€â”¬â”€â”€â”˜ â””â”€â”¬â”€â”€â”˜ â””â”€â”¬â”€â”€â”˜   â”‚
â”‚    â”‚      â”‚      â”‚      â”‚       â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚            â†“                     â”‚
â”‚     Concatenate + Conv           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Decoder              â”‚
â”‚ UpSample 4x â†’ 28x28  â”‚
â”‚ Skip connection      â”‚
â”‚ UpSample 8x â†’ 224x224â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
    MÃ¡scara de clases
    (21 canales)
```

**ASPP (Atrous Spatial Pyramid Pooling):**
- MÃºltiples convoluciones con diferentes tasas de dilataciÃ³n
- Captura contexto en mÃºltiples escalas
- Mejora la precisiÃ³n sin perder resoluciÃ³n

**Clases PASCAL VOC (DeepLabV3+):**
```
 0: background       7: cat          14: motorbike
 1: aeroplane        8: chair        15: person â­
 2: bicycle          9: cow          16: pottedplant
 3: bird            10: diningtable  17: sheep
 4: boat            11: dog          18: sofa
 5: bottle          12: horse        19: train
 6: bus             13: motorbike    20: tvmonitor
```

**Salida DeepLabV3+:**
```python
{
    'tipo': 'segmentacion_semantica',
    'clase': 'person',
    'confianza': 0.87,
    'bbox': [x, y, w, h],
    'segmentacion': mascara_clases,  # Array con IDs de clase
    'area_segmentada': 24680,
    'porcentaje': 49.2,
    'clases_detectadas': ['person', 'chair', 'bottle']
}
```

**Ventajas de cada enfoque:**

| CaracterÃ­stica | Mask R-CNN | DeepLabV3+ |
|---------------|------------|------------|
| Instancias separadas | âœ… SÃ­ | âŒ No |
| MÃºltiples objetos | âœ… Excelente | âš ï¸ Fusionados |
| Calidad de bordes | â­â­â­â­ | â­â­â­â­â­ |
| Velocidad | Media | RÃ¡pida |
| Clases disponibles | 80 (COCO) | 21 (VOC) |
| InstalaciÃ³n | DifÃ­cil (Windows) | FÃ¡cil |
| Bbox automÃ¡tico | âœ… SÃ­ | âš ï¸ Calculado |

**CuÃ¡ndo usar cada uno:**

**Mask R-CNN:**
- Necesitas distinguir instancias individuales (ej: 3 personas separadas)
- Trabajas en Linux/macOS (fÃ¡cil instalaciÃ³n)
- Necesitas las 80 clases de COCO
- PrecisiÃ³n es mÃ¡s importante que velocidad

**DeepLabV3+:**
- Solo necesitas saber QUÃ‰ objetos hay (no cuÃ¡ntos)
- Trabajas en Windows
- Necesitas alta calidad de bordes
- Velocidad es importante
- Las 21 clases de VOC son suficientes

**InstalaciÃ³n de Mask R-CNN (opcional):**

```bash
# En Linux/macOS (recomendado):
pip install torch torchvision
pip install 'git+https://github.com/facebookresearch/detectron2.git'

# En Windows (complejo):
# Requiere compilaciÃ³n con Visual Studio
# Se recomienda usar WSL o Docker
# DocumentaciÃ³n: https://detectron2.readthedocs.io/
```

**Preprocesamiento:**

Ambos modelos manejan su propio preprocesamiento internamente:

```python
# Mask R-CNN (detectron2)
# Espera BGR, lo convierte internamente
imagen_bgr = cv2.cvtColor(imagen_rgb, cv2.COLOR_RGB2BGR)
outputs = predictor(imagen_bgr)

# DeepLabV3+ (TensorFlow)
# Usa preprocesamiento de ResNet50
imagen_bgr = cv2.cvtColor(imagen_rgb, cv2.COLOR_RGB2BGR)
imagen_batch = np.expand_dims(cv2.resize(imagen_bgr, (224, 224)), axis=0)
prediccion = model.predict(imagen_batch)
```

---

## ğŸ†š ComparaciÃ³n: ClasificaciÃ³n vs. SegmentaciÃ³n

| Aspecto | ClasificaciÃ³n (VGG/ResNet) | SegmentaciÃ³n (U-Net/Mask R-CNN) |
|---------|---------------------------|----------------------------------|
| **Salida** | Etiqueta de clase | MÃ¡scara pixel por pixel |
| **InformaciÃ³n** | "QuÃ© hay" | "QuÃ© hay y DÃ“NDE estÃ¡" |
| **PrecisiÃ³n espacial** | Baja (solo imagen completa) | Alta (nivel de pÃ­xel) |
| **Velocidad** | RÃ¡pida | Media/Lenta |
| **Uso de memoria** | Bajo | Alto |
| **Clases** | 1000 (ImageNet) | Variable (21-80+) |
| **Aplicaciones** | Reconocimiento general | EdiciÃ³n, conteo, mediciÃ³n |

**Ejemplo prÃ¡ctico:**

Imagen: Persona con sombrero

**ClasificaciÃ³n (VGG16):**
```
Salida: "Cowboy Hat" (79.2% confianza)
InformaciÃ³n: Hay un sombrero vaquero en la imagen
```

**SegmentaciÃ³n (U-Net):**
```
Salida: MÃ¡scara binaria mostrando pÃ­xeles del objeto
InformaciÃ³n: 
  - Objeto estÃ¡ en coordenadas (120, 80)
  - Ocupa 15,420 pÃ­xeles (30% de la imagen)
  - Bounding box: 80x150 pÃ­xeles
  - Densidad: 85% (forma compacta)
```

---

## ğŸ“Š Resumen de Modelos Implementados

### ClasificaciÃ³n (ImageNet - 1000 clases)

1. **LeNet** - Arquitectura bÃ¡sica (no preentrenada)
2. **AlexNet** â†’ Reemplazado por VGG16 preentrenado
3. **VGG16** - Modo Caffe, BGR, 138M parÃ¡metros
4. **ResNet50** - Modo Caffe, BGR, 25M parÃ¡metros
5. **ResNet101 V2** - Modo Torch, RGB, 44M parÃ¡metros

### SegmentaciÃ³n

6. **U-Net** - ResNet50 encoder + decoder personalizado
   - Alternativa: DeepLabV3 desde TF Hub
7. **Mask R-CNN** - Detectron2 (80 clases COCO)
   - Alternativa: DeepLabV3+ (21 clases VOC)

### DetecciÃ³n de Objetos

8. **YOLO** - YOLOv8 nano (ultralytics)

---

## ğŸ“ Conceptos Clave

### Transfer Learning
Usar modelos preentrenados en ImageNet para otras tareas:
- Encoder congelado preserva caracterÃ­sticas aprendidas
- Solo se entrena el decoder/clasificador final
- Reduce drÃ¡sticamente el tiempo y datos necesarios

### Skip Connections
Conexiones que saltan capas en redes profundas:
- Resuelven el problema del gradiente que desaparece
- Preservan informaciÃ³n espacial en segmentaciÃ³n
- Combinan caracterÃ­sticas de diferentes niveles

### Atrous Convolutions (Dilated Convolutions)
Convoluciones con "agujeros" entre pÃ­xeles:
- Aumentan el campo receptivo sin aumentar parÃ¡metros
- Capturan contexto a mÃºltiples escalas
- Esenciales en DeepLabV3+

---

**Documento actualizado:** 17 de Noviembre de 2025  
**Proyecto:** Sistema de DetecciÃ³n y SegmentaciÃ³n  
**Universidad del QuindÃ­o** - VisiÃ³n Artificial

